{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2efadb",
   "metadata": {},
   "source": [
    "# Data Preparation - RAF-DB Face Expression Recognition\n",
    "\n",
    "**Project:** FER AI with BLIP Fine-tuning  \n",
    "**Dataset:** Balanced RAF-DB Dataset (7575 Grayscale)  \n",
    "**Source:** https://www.kaggle.com/datasets/dollyprajapati182/balanced-raf-db-dataset-7575-grayscale  \n",
    "**Purpose:** Import, explore, and visualize the dataset for BLIP model fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "1. Environment Setup & Configuration\n",
    "2. Data Loading\n",
    "3. Data Exploration & Statistics\n",
    "4. Data Visualization\n",
    "5. Data Quality Checks\n",
    "6. Export Metadata for MLOps Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5f15f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27625491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment (Colab vs Kaggle vs Local)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check for Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IS_COLAB = True\n",
    "    print(\"✓ Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# Check for Kaggle by looking at environment variables or file paths\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ or 'KAGGLE_DATA_PROXY_TOKEN' in os.environ:\n",
    "    IS_KAGGLE = True\n",
    "    print(\"✓ Running on Kaggle\")\n",
    "else:\n",
    "    IS_KAGGLE = False\n",
    "\n",
    "# Mount Google Drive only in Colab\n",
    "if IS_COLAB:\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        BASE_PATH = '/content/drive/MyDrive/FER_AI_Project'\n",
    "        print(f\"✓ Google Drive mounted at {BASE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to mount Google Drive: {e}\")\n",
    "        BASE_PATH = '/kaggle/working'\n",
    "        print(f\"Using fallback path: {BASE_PATH}\")\n",
    "\n",
    "elif IS_KAGGLE:\n",
    "    # Kaggle environment - use Kaggle paths\n",
    "    BASE_PATH = '/kaggle/working'\n",
    "    \n",
    "    # You might also want to check for Kaggle datasets\n",
    "    # If your data is in Kaggle datasets, you can access them at /kaggle/input/\n",
    "    print(f\"✓ Kaggle environment detected. Base path: {BASE_PATH}\")\n",
    "    \n",
    "    # Common dataset locations in Kaggle\n",
    "    DATA_INPUT_PATH = '/kaggle/input'\n",
    "    if os.path.exists(DATA_INPUT_PATH):\n",
    "        print(f\"✓ Kaggle input datasets available at: {DATA_INPUT_PATH}\")\n",
    "        \n",
    "else:\n",
    "    # Local environment\n",
    "    BASE_PATH = r'c:\\Users\\famil\\Desktop\\ghaith\\Projects\\FER_AI_Project'\n",
    "    print(f\"✓ Local machine detected. Base path: {BASE_PATH}\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "print(f\"✓ Using base path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca02555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "CONFIG = {\n",
    "    'project_name': 'FER_AI_BLIP',\n",
    "    'data_version': '1.0',\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'environment': 'colab' if IS_COLAB else 'local',\n",
    "    \n",
    "    # Paths (will be updated after download)\n",
    "    'data_root': f'{BASE_PATH}/data',\n",
    "    'raw_data_path': f'{BASE_PATH}/data/raw',  # This will be updated after kagglehub download\n",
    "    'processed_data_path': f'{BASE_PATH}/data/processed',\n",
    "    'metadata_path': f'{BASE_PATH}/data/metadata',\n",
    "    \n",
    "    # Dataset splits\n",
    "    'train_dir': 'train',\n",
    "    'val_dir': 'val',\n",
    "    'test_dir': 'test',\n",
    "    \n",
    "    # Emotion folder names\n",
    "    'emotion_folders': ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'],\n",
    "    \n",
    "    # Emotion labels mapping\n",
    "    'emotion_labels': {\n",
    "        'angry': 'Anger',\n",
    "        'disgust': 'Disgust',\n",
    "        'fear': 'Fear',\n",
    "        'happy': 'Happiness',\n",
    "        'neutral': 'Neutral',\n",
    "        'sad': 'Sadness',\n",
    "        'surprise': 'Surprise'\n",
    "    },\n",
    "    \n",
    "    # Numeric mapping for model training\n",
    "    'emotion_to_id': {\n",
    "        'angry': 0,\n",
    "        'disgust': 1,\n",
    "        'fear': 2,\n",
    "        'happy': 3,\n",
    "        'neutral': 4,\n",
    "        'sad': 5,\n",
    "        'surprise': 6\n",
    "    },\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"Environment: {CONFIG['environment'].upper()}\")\n",
    "print(f\"Base Path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "directories = [\n",
    "    CONFIG['data_root'],\n",
    "    CONFIG['processed_data_path'],\n",
    "    CONFIG['metadata_path']\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ Created/verified: {directory}\")\n",
    "\n",
    "print(\"\\nDirectory structure ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc26243",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa646f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-download dataset from Kaggle\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "dataset_name = \"dollyprajapati182/balanced-raf-db-dataset-7575-grayscale\"\n",
    "print(f\"Downloading dataset: {dataset_name}\")\n",
    "download_path = kagglehub.dataset_download(dataset_name)\n",
    "print(f\"✓ Dataset downloaded to: {download_path}\")\n",
    "\n",
    "# Update raw data path to the downloaded location\n",
    "CONFIG['raw_data_path'] = download_path\n",
    "print(f\"\\nDataset location: {CONFIG['raw_data_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfac019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if dataset is mounted/available\n",
    "def check_dataset_availability(data_path):\n",
    "    \"\"\"\n",
    "    Check if the dataset is properly downloaded and structured\n",
    "    \n",
    "    Returns:\n",
    "        dict: Status of each split (train/val/test) and emotion folders\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    status = {\n",
    "        'dataset_found': False,\n",
    "        'splits': {},\n",
    "        'total_images': 0,\n",
    "        'missing_folders': []\n",
    "    }\n",
    "    \n",
    "    base_path = Path(data_path)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"❌ Dataset path not found: {data_path}\")\n",
    "        return status\n",
    "    \n",
    "    print(f\"✓ Dataset path exists: {data_path}\\n\")\n",
    "    \n",
    "    # Check each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = base_path / split\n",
    "        status['splits'][split] = {\n",
    "            'exists': split_path.exists(),\n",
    "            'emotions': {},\n",
    "            'total': 0\n",
    "        }\n",
    "        \n",
    "        if split_path.exists():\n",
    "            print(f\"✓ {split.upper()} split found\")\n",
    "            \n",
    "            # Check emotion folders\n",
    "            for emotion in CONFIG['emotion_folders']:\n",
    "                emotion_path = split_path / emotion\n",
    "                if emotion_path.exists():\n",
    "                    image_count = len(list(emotion_path.glob('*.jpg')) + \n",
    "                                    list(emotion_path.glob('*.png')) + \n",
    "                                    list(emotion_path.glob('*.jpeg')))\n",
    "                    status['splits'][split]['emotions'][emotion] = image_count\n",
    "                    status['splits'][split]['total'] += image_count\n",
    "                    status['total_images'] += image_count\n",
    "                else:\n",
    "                    status['missing_folders'].append(f\"{split}/{emotion}\")\n",
    "            \n",
    "            print(f\"  - Total images: {status['splits'][split]['total']}\")\n",
    "        else:\n",
    "            print(f\"❌ {split.upper()} split NOT found\")\n",
    "    \n",
    "    status['dataset_found'] = all(status['splits'][s]['exists'] for s in ['train', 'val', 'test'])\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset Status: {'✓ READY' if status['dataset_found'] else '❌ INCOMPLETE'}\")\n",
    "    print(f\"Total Images Found: {status['total_images']}\")\n",
    "    if status['missing_folders']:\n",
    "        print(f\"Missing Folders: {len(status['missing_folders'])}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "# Check if dataset is available\n",
    "dataset_status = check_dataset_availability(CONFIG['raw_data_path'])\n",
    "\n",
    "if not dataset_status['dataset_found']:\n",
    "    print(\"⚠️ Dataset not complete. Please ensure it's properly downloaded.\")\n",
    "else:\n",
    "    print(\"✓ Dataset is ready for processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_metadata(data_path, split_name):\n",
    "    \"\"\"\n",
    "    Load metadata for a dataset split (train/val/test)\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the dataset\n",
    "        split_name: Name of the split (train, val, test)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with image paths and labels\n",
    "    \"\"\"\n",
    "    split_path = Path(data_path) / split_name\n",
    "    \n",
    "    if not split_path.exists():\n",
    "        print(f\"⚠️  Warning: {split_path} does not exist!\")\n",
    "        return pd.DataFrame(columns=['image_path', 'emotion_folder', 'emotion_id', 'emotion_label', 'split'])\n",
    "    \n",
    "    image_data = []\n",
    "    \n",
    "    # Iterate through emotion folders (angry, disgust, fear, etc.)\n",
    "    for emotion_folder_name in CONFIG['emotion_folders']:\n",
    "        emotion_folder = split_path / emotion_folder_name\n",
    "        \n",
    "        if emotion_folder.exists():\n",
    "            # Get all image files\n",
    "            image_files = list(emotion_folder.glob('*.jpg')) + \\\n",
    "                         list(emotion_folder.glob('*.png')) + \\\n",
    "                         list(emotion_folder.glob('*.jpeg'))\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                image_data.append({\n",
    "                    'image_path': str(img_path),\n",
    "                    'filename': img_path.name,\n",
    "                    'emotion_folder': emotion_folder_name,\n",
    "                    'emotion_id': CONFIG['emotion_to_id'][emotion_folder_name],\n",
    "                    'emotion_label': CONFIG['emotion_labels'][emotion_folder_name],\n",
    "                    'split': split_name\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(image_data)\n",
    "    print(f\"✓ Loaded {len(df)} images from {split_name} split\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Data loading function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b046bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dataset splits\n",
    "print(\"Loading dataset splits...\\n\")\n",
    "\n",
    "train_df = load_dataset_metadata(CONFIG['raw_data_path'], CONFIG['train_dir'])\n",
    "val_df = load_dataset_metadata(CONFIG['raw_data_path'], CONFIG['val_dir'])\n",
    "test_df = load_dataset_metadata(CONFIG['raw_data_path'], CONFIG['test_dir'])\n",
    "\n",
    "# Combine all splits\n",
    "full_dataset_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Total dataset size: {len(full_dataset_df)} images\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f17b6",
   "metadata": {},
   "source": [
    "## 3. Data Exploration & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55125a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of the dataset\n",
    "print(\"Dataset Sample:\")\n",
    "display(full_dataset_df.head(10))\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(full_dataset_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68375bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution statistics by split\n",
    "print(\"Distribution by Split:\")\n",
    "split_dist = full_dataset_df['split'].value_counts().sort_index()\n",
    "print(split_dist)\n",
    "print(f\"\\nPercentages:\")\n",
    "print(split_dist / len(full_dataset_df) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96172b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution statistics by emotion\n",
    "print(\"Distribution by Emotion (Overall):\")\n",
    "emotion_dist = full_dataset_df['emotion_label'].value_counts().sort_index()\n",
    "print(emotion_dist)\n",
    "\n",
    "print(\"\\nDistribution by Emotion (Per Split):\")\n",
    "split_emotion_dist = pd.crosstab(full_dataset_df['split'], \n",
    "                                  full_dataset_df['emotion_label'])\n",
    "display(split_emotion_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image analysis\n",
    "print(\"Analyzing sample images...\\n\")\n",
    "\n",
    "def analyze_image_properties(df, num_samples=100):\n",
    "    \"\"\"\n",
    "    Analyze image properties (dimensions, size, etc.)\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    sample_df = df.sample(min(num_samples, len(df)), random_state=CONFIG['random_seed'])\n",
    "    \n",
    "    properties = {\n",
    "        'width': [],\n",
    "        'height': [],\n",
    "        'channels': [],\n",
    "        'file_size_kb': []\n",
    "    }\n",
    "    \n",
    "    for img_path in sample_df['image_path']:\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            width, height = img.size\n",
    "            channels = len(img.getbands())\n",
    "            file_size = os.path.getsize(img_path) / 1024  # KB\n",
    "            \n",
    "            properties['width'].append(width)\n",
    "            properties['height'].append(height)\n",
    "            properties['channels'].append(channels)\n",
    "            properties['file_size_kb'].append(file_size)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(properties)\n",
    "\n",
    "image_props_df = analyze_image_properties(full_dataset_df, num_samples=100)\n",
    "\n",
    "if image_props_df is not None:\n",
    "    print(\"Image Properties Statistics:\")\n",
    "    display(image_props_df.describe())\n",
    "    \n",
    "    print(f\"\\nUnique image dimensions:\")\n",
    "    print(image_props_df.groupby(['width', 'height']).size().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57000a1",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution across splits\n",
    "if len(full_dataset_df) == 0:\n",
    "    print(\"⚠️ No data available to visualize. Please ensure the dataset is downloaded and placed in the correct directory.\")\n",
    "    print(f\"Expected location: {CONFIG['raw_data_path']}\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Overall distribution\n",
    "    emotion_counts = full_dataset_df['emotion_label'].value_counts()\n",
    "    axes[0].bar(emotion_counts.index, emotion_counts.values, color='steelblue', alpha=0.8)\n",
    "    axes[0].set_title('Overall Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Emotion')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (emotion, count) in enumerate(emotion_counts.items()):\n",
    "        axes[0].text(i, count + 20, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Distribution by split\n",
    "    split_emotion_pivot = full_dataset_df.groupby(['split', 'emotion_label']).size().unstack(fill_value=0)\n",
    "    split_emotion_pivot.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "    axes[1].set_title('Emotion Distribution by Split', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Split')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].tick_params(axis='x', rotation=0)\n",
    "    axes[1].legend(title='Emotion', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Class distribution visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each emotion class\n",
    "def visualize_emotion_samples(df, num_samples_per_emotion=3):\n",
    "    \"\"\"\n",
    "    Display sample images for each emotion class\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    emotions = sorted(df['emotion_label'].unique())\n",
    "    num_emotions = len(emotions)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_emotions, num_samples_per_emotion, \n",
    "                            figsize=(num_samples_per_emotion * 3, num_emotions * 3))\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        emotion_df = df[df['emotion_label'] == emotion]\n",
    "        samples = emotion_df.sample(min(num_samples_per_emotion, len(emotion_df)), \n",
    "                                   random_state=CONFIG['random_seed'])\n",
    "        \n",
    "        for j, (_, row) in enumerate(samples.iterrows()):\n",
    "            ax = axes[i, j] if num_emotions > 1 else axes[j]\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(row['image_path'])\n",
    "                ax.imshow(img, cmap='gray')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                if j == 0:\n",
    "                    ax.set_title(f\"{emotion}\\n{row['filename']}\", \n",
    "                               fontsize=10, fontweight='bold', loc='left')\n",
    "                else:\n",
    "                    ax.set_title(row['filename'], fontsize=9, loc='left')\n",
    "            except Exception as e:\n",
    "                ax.text(0.5, 0.5, f'Error loading\\nimage', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images by Emotion Class', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Displaying sample images from each emotion class...\\n\")\n",
    "visualize_emotion_samples(full_dataset_df, num_samples_per_emotion=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b053d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image dimension distribution\n",
    "if image_props_df is not None and len(image_props_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Width distribution\n",
    "    axes[0, 0].hist(image_props_df['width'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_title('Image Width Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Width (pixels)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Height distribution\n",
    "    axes[0, 1].hist(image_props_df['height'], bins=20, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_title('Image Height Distribution', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Height (pixels)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # File size distribution\n",
    "    axes[1, 0].hist(image_props_df['file_size_kb'], bins=20, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_title('File Size Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('File Size (KB)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Channels distribution\n",
    "    channel_counts = image_props_df['channels'].value_counts().sort_index()\n",
    "    axes[1, 1].bar(channel_counts.index, channel_counts.values, color='orange', alpha=0.7)\n",
    "    axes[1, 1].set_title('Image Channels Distribution', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Number of Channels')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Image properties visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c8d52",
   "metadata": {},
   "source": [
    "## 6. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the entire dataset (UNCOMMENT TO RUN)\n",
    "# This will process all images and save them to the processed directory\n",
    "\n",
    "# OPTION 1: Process without normalization (save raw cropped faces)\n",
    "# processing_stats = process_and_save_dataset(\n",
    "#     full_dataset_df, \n",
    "#     CONFIG['processed_data_path'],\n",
    "#     target_size=(224, 224),\n",
    "#     save_normalized=False\n",
    "# )\n",
    "\n",
    "# OPTION 2: Process small sample for testing (recommended first)\n",
    "# if len(full_dataset_df) > 0:\n",
    "#     sample_df = full_dataset_df.sample(min(100, len(full_dataset_df)), random_state=CONFIG['random_seed'])\n",
    "#     processing_stats = process_and_save_dataset(\n",
    "#         sample_df, \n",
    "#         CONFIG['processed_data_path'] + '_sample',\n",
    "#         target_size=(224, 224),\n",
    "#         save_normalized=False\n",
    "#     )\n",
    "\n",
    "print(\"Uncomment the code above to process the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save preprocessed dataset\n",
    "def process_and_save_dataset(df, output_base_path, target_size=(224, 224), save_normalized=False):\n",
    "    \"\"\"\n",
    "    Process entire dataset and save to disk\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with image metadata\n",
    "        output_base_path: Base path to save processed images\n",
    "        target_size: Target image size\n",
    "        save_normalized: Whether to save normalized or raw cropped images\n",
    "    \n",
    "    Returns:\n",
    "        Statistics about processing\n",
    "    \"\"\"\n",
    "    output_base_path = Path(output_base_path)\n",
    "    output_base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    stats = {\n",
    "        'total': len(df),\n",
    "        'processed': 0,\n",
    "        'failed': 0,\n",
    "        'no_face': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing {stats['total']} images...\")\n",
    "    print(f\"Output path: {output_base_path}\\n\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
    "        try:\n",
    "            # Create output directory structure: split/emotion/\n",
    "            output_dir = output_base_path / row['split'] / row['emotion_folder']\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Process image\n",
    "            processed_image = preprocess_image_for_training(\n",
    "                row['image_path'], \n",
    "                target_size=target_size,\n",
    "                normalize=save_normalized\n",
    "            )\n",
    "            \n",
    "            if processed_image is not None:\n",
    "                # Save processed image\n",
    "                output_path = output_dir / row['filename']\n",
    "                \n",
    "                if save_normalized:\n",
    "                    # Denormalize for saving\n",
    "                    img_to_save = ((processed_image * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]) * 255\n",
    "                    img_to_save = np.clip(img_to_save, 0, 255).astype(np.uint8)\n",
    "                else:\n",
    "                    img_to_save = processed_image\n",
    "                \n",
    "                # Convert RGB to BGR for OpenCV saving\n",
    "                img_to_save = cv2.cvtColor(img_to_save, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(str(output_path), img_to_save)\n",
    "                \n",
    "                stats['processed'] += 1\n",
    "            else:\n",
    "                stats['no_face'] += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            stats['failed'] += 1\n",
    "            if stats['failed'] <= 5:  # Show first 5 errors\n",
    "                print(f\"Error processing {row['filename']}: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing Complete!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total images: {stats['total']}\")\n",
    "    print(f\"Successfully processed: {stats['processed']}\")\n",
    "    print(f\"No face detected: {stats['no_face']}\")\n",
    "    print(f\"Failed: {stats['failed']}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"✓ Batch processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a979476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Preprocessing Pipeline\n",
    "def preprocess_image_for_training(image_path, target_size=(224, 224), normalize=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for training:\n",
    "    1. Detect and crop face\n",
    "    2. Resize to target size\n",
    "    3. Normalize pixel values\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        target_size: Target image size (width, height)\n",
    "        normalize: Whether to normalize the image\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image ready for model input\n",
    "    \"\"\"\n",
    "    # Step 1: Detect and crop face\n",
    "    face_image = detect_and_crop_face(image_path, target_size=target_size)\n",
    "    \n",
    "    if face_image is None:\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Normalize (if requested)\n",
    "    if normalize:\n",
    "        face_image = normalize_image(face_image)\n",
    "    \n",
    "    return face_image\n",
    "\n",
    "print(\"✓ Complete preprocessing pipeline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test face detection on sample images\n",
    "print(\"Testing face detection on sample images...\\n\")\n",
    "\n",
    "if len(full_dataset_df) > 0:\n",
    "    # Sample 5 images\n",
    "    sample_images = full_dataset_df.sample(min(5, len(full_dataset_df)), random_state=CONFIG['random_seed'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    \n",
    "    for idx, (_, row) in enumerate(sample_images.iterrows()):\n",
    "        # Original image\n",
    "        original = Image.open(row['image_path'])\n",
    "        axes[0, idx].imshow(original, cmap='gray')\n",
    "        axes[0, idx].set_title(f\"Original\\n{row['emotion_label']}\", fontsize=9)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Processed image (face detected + cropped)\n",
    "        processed = detect_and_crop_face(row['image_path'])\n",
    "        if processed is not None:\n",
    "            axes[1, idx].imshow(processed)\n",
    "            axes[1, idx].set_title(\"Cropped Face\", fontsize=9)\n",
    "        else:\n",
    "            axes[1, idx].text(0.5, 0.5, 'No face\\ndetected', \n",
    "                            ha='center', va='center', transform=axes[1, idx].transAxes)\n",
    "        axes[1, idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Face Detection & Cropping Test', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Face detection test complete\")\n",
    "else:\n",
    "    print(\"⚠️ No images available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33752b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Function\n",
    "def normalize_image(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Normalize image using ImageNet statistics (standard for pretrained models)\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (RGB, 0-255)\n",
    "        mean: Mean values for each channel\n",
    "        std: Standard deviation values for each channel\n",
    "    \n",
    "    Returns:\n",
    "        Normalized image\n",
    "    \"\"\"\n",
    "    # Convert to float and scale to [0, 1]\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Handle grayscale images (convert to RGB)\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack([image] * 3, axis=-1)\n",
    "    \n",
    "    # Normalize using mean and std\n",
    "    image = (image - mean) / std\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"✓ Normalization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Detection and Cropping Function\n",
    "def detect_and_crop_face(image_path, target_size=(224, 224), margin=20):\n",
    "    \"\"\"\n",
    "    Detect and crop face from image using MTCNN\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        target_size: Output image size (width, height)\n",
    "        margin: Margin around detected face in pixels\n",
    "    \n",
    "    Returns:\n",
    "        Cropped and resized face image, or None if no face detected\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert BGR to RGB (MTCNN expects RGB)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        detections = detector.detect_faces(img_rgb)\n",
    "        \n",
    "        if len(detections) == 0:\n",
    "            # No face detected, resize original image\n",
    "            img_resized = cv2.resize(img_rgb, target_size)\n",
    "            return img_resized\n",
    "        \n",
    "        # Get the first (largest) face\n",
    "        detection = detections[0]\n",
    "        x, y, width, height = detection['box']\n",
    "        \n",
    "        # Add margin\n",
    "        x = max(0, x - margin)\n",
    "        y = max(0, y - margin)\n",
    "        width = min(img_rgb.shape[1] - x, width + 2 * margin)\n",
    "        height = min(img_rgb.shape[0] - y, height + 2 * margin)\n",
    "        \n",
    "        # Crop face\n",
    "        face = img_rgb[y:y+height, x:x+width]\n",
    "        \n",
    "        # Resize to target size\n",
    "        face_resized = cv2.resize(face, target_size)\n",
    "        \n",
    "        return face_resized\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Face detection function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c503d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for face detection\n",
    "!pip install opencv-python-headless mtcnn -q\n",
    "\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "# Initialize face detector\n",
    "detector = MTCNN()\n",
    "\n",
    "print(\"✓ Face detection libraries installed and initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728215bd",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing Pipeline\n",
    "\n",
    "This section prepares images for training:\n",
    "1. **Face Detection & Cropping**: Detect and crop faces from images\n",
    "2. **Normalization**: Standardize pixel values for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality validation\n",
    "print(\"Running Data Quality Checks...\\n\")\n",
    "\n",
    "quality_report = {\n",
    "    'total_images': len(full_dataset_df),\n",
    "    'missing_files': 0,\n",
    "    'corrupted_files': 0,\n",
    "    'duplicate_files': 0,\n",
    "    'issues': []\n",
    "}\n",
    "\n",
    "# Check for missing files\n",
    "print(\"1. Checking for missing files...\")\n",
    "for idx, row in full_dataset_df.iterrows():\n",
    "    if not os.path.exists(row['image_path']):\n",
    "        quality_report['missing_files'] += 1\n",
    "        quality_report['issues'].append(f\"Missing: {row['image_path']}\")\n",
    "\n",
    "print(f\"   Missing files: {quality_report['missing_files']}\")\n",
    "\n",
    "# Check for corrupted files (sample)\n",
    "print(\"\\n2. Checking for corrupted files (sampling 100 images)...\")\n",
    "sample_check = full_dataset_df.sample(min(100, len(full_dataset_df)), random_state=CONFIG['random_seed'])\n",
    "\n",
    "for idx, row in sample_check.iterrows():\n",
    "    try:\n",
    "        img = Image.open(row['image_path'])\n",
    "        img.verify()  # Verify image integrity\n",
    "    except Exception as e:\n",
    "        quality_report['corrupted_files'] += 1\n",
    "        quality_report['issues'].append(f\"Corrupted: {row['image_path']} - {str(e)}\")\n",
    "\n",
    "print(f\"   Corrupted files (in sample): {quality_report['corrupted_files']}\")\n",
    "\n",
    "# Check for duplicate filenames\n",
    "print(\"\\n3. Checking for duplicate filenames...\")\n",
    "duplicate_filenames = full_dataset_df['filename'].duplicated().sum()\n",
    "quality_report['duplicate_files'] = duplicate_filenames\n",
    "print(f\"   Duplicate filenames: {duplicate_filenames}\")\n",
    "\n",
    "# Check class balance\n",
    "print(\"\\n4. Checking class balance...\")\n",
    "emotion_counts = full_dataset_df['emotion_label'].value_counts()\n",
    "max_count = emotion_counts.max()\n",
    "min_count = emotion_counts.min()\n",
    "imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "\n",
    "print(f\"   Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 2.0:\n",
    "    quality_report['issues'].append(f\"Warning: Class imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Data Quality Summary:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total Images: {quality_report['total_images']}\")\n",
    "print(f\"Missing Files: {quality_report['missing_files']}\")\n",
    "print(f\"Corrupted Files: {quality_report['corrupted_files']}\")\n",
    "print(f\"Duplicate Files: {quality_report['duplicate_files']}\")\n",
    "print(f\"\\nIssues Found: {len(quality_report['issues'])}\")\n",
    "\n",
    "if quality_report['issues']:\n",
    "    print(\"\\nIssue Details:\")\n",
    "    for issue in quality_report['issues'][:10]:  # Show first 10\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"\\n✓ No critical issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542eeb4",
   "metadata": {},
   "source": [
    "## 7. Export Metadata for Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset metadata for MLOps tracking\n",
    "print(\"Exporting metadata for MLOps pipeline...\\n\")\n",
    "\n",
    "# Export full dataset metadata\n",
    "metadata_file = Path(CONFIG['metadata_path']) / f\"dataset_metadata_{CONFIG['timestamp']}.csv\"\n",
    "full_dataset_df.to_csv(metadata_file, index=False)\n",
    "print(f\"✓ Dataset metadata saved: {metadata_file}\")\n",
    "\n",
    "# Export dataset statistics\n",
    "dataset_stats = {\n",
    "    'metadata': {\n",
    "        'project_name': CONFIG['project_name'],\n",
    "        'data_version': CONFIG['data_version'],\n",
    "        'timestamp': CONFIG['timestamp'],\n",
    "        'total_images': len(full_dataset_df)\n",
    "    },\n",
    "    'splits': {\n",
    "        'train': len(train_df),\n",
    "        'val': len(val_df),\n",
    "        'test': len(test_df)\n",
    "    },\n",
    "    'emotion_distribution': full_dataset_df['emotion_label'].value_counts().to_dict(),\n",
    "    'quality_report': quality_report,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "if image_props_df is not None and len(image_props_df) > 0:\n",
    "    dataset_stats['image_properties'] = {\n",
    "        'mean_width': float(image_props_df['width'].mean()),\n",
    "        'mean_height': float(image_props_df['height'].mean()),\n",
    "        'mean_file_size_kb': float(image_props_df['file_size_kb'].mean()),\n",
    "        'channels': int(image_props_df['channels'].mode()[0])\n",
    "    }\n",
    "\n",
    "stats_file = Path(CONFIG['metadata_path']) / f\"dataset_stats_{CONFIG['timestamp']}.json\"\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(dataset_stats, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Dataset statistics saved: {stats_file}\")\n",
    "\n",
    "# Export split-specific CSVs for training pipeline\n",
    "train_df.to_csv(Path(CONFIG['metadata_path']) / 'train_metadata.csv', index=False)\n",
    "val_df.to_csv(Path(CONFIG['metadata_path']) / 'val_metadata.csv', index=False)\n",
    "test_df.to_csv(Path(CONFIG['metadata_path']) / 'test_metadata.csv', index=False)\n",
    "\n",
    "print(f\"✓ Split-specific metadata saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✓ All metadata exported successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef0d25",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Current Status:\n",
    "- ✓ Dataset loaded and validated\n",
    "- ✓ Data exploration and visualization completed\n",
    "- ✓ Quality checks performed\n",
    "- ✓ Metadata exported for MLOps pipeline\n",
    "\n",
    "### Next Steps for BLIP Fine-tuning:\n",
    "1. **Data Preprocessing**: Resize images, normalize, augmentation\n",
    "2. **Feature Engineering**: Prepare emotion labels for BLIP format\n",
    "3. **Model Setup**: Configure BLIP model for fine-tuning\n",
    "4. **Training Pipeline**: Implement training with MLOps tracking\n",
    "5. **Evaluation**: Model performance on test set\n",
    "6. **Deployment**: Model versioning and serving\n",
    "\n",
    "### MLOps Considerations:\n",
    "- Data versioning with DVC or similar\n",
    "- Experiment tracking with MLflow/Weights & Biases\n",
    "- Model registry for version control\n",
    "- CI/CD pipeline for automated training\n",
    "- Monitoring and logging infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9333da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" \" * 15 + \"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: RAF-DB Balanced (Grayscale)\")\n",
    "print(f\"Total Images: {len(full_dataset_df):,}\")\n",
    "print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")\n",
    "print(f\"\\nEmotion Classes: {len(CONFIG['emotion_labels'])}\")\n",
    "print(f\"Format: Grayscale\")\n",
    "print(f\"\\nMetadata Location: {CONFIG['metadata_path']}\")\n",
    "print(f\"Timestamp: {CONFIG['timestamp']}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Save dataset metadata to Google Drive for training notebook\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Saving dataset metadata to Google Drive for training notebook...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create metadata directory on Google Drive\n",
    "metadata_dir = Path(BASE_PATH) / 'data' / 'metadata'\n",
    "metadata_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save full dataset metadata as CSV\n",
    "dataset_csv_path = metadata_dir / 'dataset_metadata.csv'\n",
    "full_dataset_df.to_csv(dataset_csv_path, index=False)\n",
    "print(f\"✓ Full dataset metadata: {dataset_csv_path}\")\n",
    "\n",
    "# Save split-specific metadata\n",
    "train_df.to_csv(metadata_dir / 'train_metadata.csv', index=False)\n",
    "val_df.to_csv(metadata_dir / 'val_metadata.csv', index=False)\n",
    "test_df.to_csv(metadata_dir / 'test_metadata.csv', index=False)\n",
    "print(f\"✓ Split-specific metadata saved\")\n",
    "\n",
    "# Save dataset summary JSON\n",
    "summary_json = {\n",
    "    'total_images': len(full_dataset_df),\n",
    "    'train_images': len(train_df),\n",
    "    'val_images': len(val_df),\n",
    "    'test_images': len(test_df),\n",
    "    'emotions': CONFIG['emotion_labels'],\n",
    "    'timestamp': CONFIG['timestamp'],\n",
    "    'data_location': BASE_PATH\n",
    "}\n",
    "\n",
    "with open(metadata_dir / 'dataset_summary.json', 'w') as f:\n",
    "    json.dump(summary_json, f, indent=2)\n",
    "\n",
    "print(f\"✓ Dataset summary saved\")\n",
    "print(f\"\\n✓ All metadata saved to Google Drive: {metadata_dir}\")\n",
    "print(f\"\\nNext step: Run 02_blip_training.ipynb to train BLIP on this dataset\")\n",
    "print(\"Ready for BLIP fine-tuning pipeline!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
