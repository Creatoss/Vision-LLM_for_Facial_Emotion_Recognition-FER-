{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ed9d8a",
   "metadata": {},
   "source": [
    "# BLIP Fine-tuning for Facial Expression Recognition - Google Drive Workflow\n",
    "\n",
    "## ðŸš€ Quick Start (Google Drive Workflow)\n",
    "\n",
    "### Step 1: Run Data Preparation\n",
    "```\n",
    "Run: 01_data_preparation.ipynb\n",
    "Output: Saves metadata to /data/metadata/ on Google Drive\n",
    "```\n",
    "\n",
    "### Step 2: Run Training (This Notebook)\n",
    "```\n",
    "Run: 02_blip_training.ipynb\n",
    "Input: Loads metadata from Google Drive (/data/metadata/)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26b721",
   "metadata": {},
   "source": [
    "# BLIP Model Fine-tuning for Facial Expression Recognition\n",
    "\n",
    "**Project:** FER AI with BLIP  \n",
    "**Dataset:** RAF-DB (Balanced, Grayscale)  \n",
    "**Task:** Fine-tune BLIP for emotion classification  \n",
    "**Environment:** Google Colab (GPU recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "1. Environment Setup & Model Loading\n",
    "2. Dataset Preparation (PyTorch DataLoader)\n",
    "3. Model Configuration\n",
    "4. Training Loop\n",
    "5. Evaluation & Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1c7a0",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3945f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install torch torchvision -q\n",
    "!pip install transformers datasets -q\n",
    "!pip install opencv-python-headless mtcnn -q\n",
    "!pip install tqdm tensorboard -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoProcessor\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ“ GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ“ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d784138",
   "metadata": {},
   "source": [
    "## 2. Configuration & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IS_COLAB = True\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = '/content/drive/MyDrive/FER_AI_Project'\n",
    "except:\n",
    "    IS_COLAB = False\n",
    "    BASE_PATH = r'c:\\Users\\famil\\Desktop\\ghaith\\Projects\\FER_AI_Project'\n",
    "\n",
    "print(f\"Environment: {'Colab' if IS_COLAB else 'Local'}\")\n",
    "print(f\"Base Path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96343e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'project_name': 'FER_AI_BLIP',\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    \n",
    "    # Paths\n",
    "    'data_root': f'{BASE_PATH}/data',\n",
    "    'raw_data_path': f'{BASE_PATH}/data/raw',\n",
    "    'processed_data_path': f'{BASE_PATH}/data/processed',\n",
    "    'models_path': f'{BASE_PATH}/models',\n",
    "    'logs_path': f'{BASE_PATH}/logs',\n",
    "    \n",
    "    # Dataset\n",
    "    'emotion_labels': {\n",
    "        'angry': 'Anger',\n",
    "        'disgust': 'Disgust',\n",
    "        'fear': 'Fear',\n",
    "        'happy': 'Happiness',\n",
    "        'neutral': 'Neutral',\n",
    "        'sad': 'Sadness',\n",
    "        'surprise': 'Surprise'\n",
    "    },\n",
    "    \n",
    "    # Model\n",
    "    'model_name': 'Salesforce/blip-image-captioning-base',\n",
    "    'image_size': (64, 64),  # Reduced for GPU memory savings\n",
    "    \n",
    "    # Training (optimized for T4 GPU, max 4 hours)\n",
    "    'batch_size': 2,  # Minimal batch size for T4 GPU memory\n",
    "    'num_epochs': 6,  # Reduced from 10 for 4-hour constraint\n",
    "    'learning_rate': 1e-4,  # Slightly higher LR for faster convergence\n",
    "    'warmup_steps': 300,  # Fewer warmup steps\n",
    "    'max_grad_norm': 1.0,\n",
    "    'weight_decay': 0.01,\n",
    "    'random_seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'accumulation_steps': 16,  # High accumulation (effective batch=32)\n",
    "    'log_interval': 10,  # Log every N batches\n",
    "    'use_fp16': True  # Mixed precision for memory efficiency\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "Path(CONFIG['models_path']).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG['logs_path']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(json.dumps({k: v for k, v in CONFIG.items() if not k.endswith('_path')}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac4f7e",
   "metadata": {},
   "source": [
    "## 3. Load Dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ea0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Google Drive (prepared by 01_data_preparation.ipynb)\n",
    "print(\"=\"*70)\n",
    "print(\"Loading dataset from Google Drive...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Check for metadata saved by data preparation notebook\n",
    "metadata_path = Path(BASE_PATH) / 'data' / 'metadata'\n",
    "dataset_metadata_csv = metadata_path / 'dataset_metadata.csv'\n",
    "\n",
    "if dataset_metadata_csv.exists():\n",
    "    # Load preprocessed metadata from Google Drive\n",
    "    print(f\"âœ“ Found metadata on Google Drive: {dataset_metadata_csv}\")\n",
    "    dataset_df = pd.read_csv(dataset_metadata_csv)\n",
    "    print(f\"âœ“ Loaded {len(dataset_df)} images from saved metadata\\n\")\n",
    "    \n",
    "    # Ensure required columns exist (handle different column naming from data prep)\n",
    "    required_columns = ['image_path', 'emotion', 'emotion_label', 'split']\n",
    "    \n",
    "    # Map column names if needed\n",
    "    if 'emotion_folder' in dataset_df.columns and 'emotion' not in dataset_df.columns:\n",
    "        dataset_df['emotion'] = dataset_df['emotion_folder']\n",
    "    \n",
    "    # Verify all required columns present\n",
    "    missing_cols = [col for col in required_columns if col not in dataset_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"âš  Warning: Missing columns {missing_cols} in metadata\")\n",
    "        print(\"  Regenerating from image paths...\\n\")\n",
    "        \n",
    "        # Extract emotion from path if missing\n",
    "        if 'emotion' not in dataset_df.columns:\n",
    "            dataset_df['emotion'] = dataset_df['image_path'].apply(\n",
    "                lambda x: Path(x).parent.name\n",
    "            )\n",
    "        \n",
    "        # Map to emotion labels if missing\n",
    "        if 'emotion_label' not in dataset_df.columns:\n",
    "            dataset_df['emotion_label'] = dataset_df['emotion'].map(CONFIG['emotion_labels'])\n",
    "        \n",
    "        # Extract split from path if missing\n",
    "        if 'split' not in dataset_df.columns:\n",
    "            dataset_df['split'] = dataset_df['image_path'].apply(\n",
    "                lambda x: Path(x).parent.parent.name\n",
    "            )\n",
    "    \n",
    "    # Update CONFIG with the raw data path from metadata\n",
    "    if len(dataset_df) > 0:\n",
    "        # Extract base path from first image path\n",
    "        first_image = dataset_df.iloc[0]['image_path']\n",
    "        # Example: /kaggle/input/balanced-raf-db-dataset-7575-grayscale/train/angry/image.jpg\n",
    "        # Extract: /kaggle/input/balanced-raf-db-dataset-7575-grayscale\n",
    "        base_path = Path(first_image).parents[2]  # Go up 2 levels from emotion/split\n",
    "        CONFIG['raw_data_path'] = str(base_path)\n",
    "        print(f\"âœ“ Dataset base path: {base_path}\\n\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: Download from Kaggle if metadata not found\n",
    "    print(\"âš  Metadata not found on Google Drive\")\n",
    "    print(\"  Please run 01_data_preparation.ipynb first!\\n\")\n",
    "    print(\"Fallback: Downloading dataset from Kaggle...\\n\")\n",
    "    \n",
    "    import kagglehub\n",
    "    \n",
    "    # Download latest version\n",
    "    dataset_name = \"dollyprajapati182/balanced-raf-db-dataset-7575-grayscale\"\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    download_path = kagglehub.dataset_download(dataset_name)\n",
    "    print(f\"âœ“ Dataset downloaded to: {download_path}\\n\")\n",
    "    \n",
    "    # Map kagglehub download path to our config\n",
    "    CONFIG['raw_data_path'] = download_path\n",
    "    \n",
    "    # Load dataset metadata from raw data\n",
    "    print(\"Scanning dataset files...\\n\")\n",
    "    \n",
    "    def load_dataset_metadata(data_path):\n",
    "        \"\"\"\n",
    "        Load dataset metadata from raw data (train/val/test structure)\n",
    "        \"\"\"\n",
    "        data_path = Path(data_path)\n",
    "        image_data = []\n",
    "        \n",
    "        emotion_folders = list(CONFIG['emotion_labels'].keys())\n",
    "        \n",
    "        # Check if data has train/val/test subdirectories\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_path = data_path / split\n",
    "            if not split_path.exists():\n",
    "                continue\n",
    "            \n",
    "            # Look for emotion folders\n",
    "            for emotion in emotion_folders:\n",
    "                emotion_path = split_path / emotion\n",
    "                if emotion_path.exists():\n",
    "                    for img_path in emotion_path.glob('*.jpg'):\n",
    "                        image_data.append({\n",
    "                            'image_path': str(img_path),\n",
    "                            'emotion': emotion,\n",
    "                            'emotion_label': CONFIG['emotion_labels'][emotion],\n",
    "                            'split': split\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(image_data)\n",
    "    \n",
    "    dataset_df = load_dataset_metadata(CONFIG['raw_data_path'])\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dataset Loaded Successfully\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "print(f\"âœ“ Total images: {len(dataset_df)}\")\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(dataset_df['split'].value_counts())\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "print(dataset_df['emotion_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed448da",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209db1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable face detection to save GPU memory (dataset already has cropped faces)\n",
    "# detector = MTCNN()  # Disabled - uses significant GPU memory\n",
    "\n",
    "class FERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Facial Expression Recognition\n",
    "    Loads images and prepares for BLIP model (no face detection needed)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, processor, config, split='train'):\n",
    "        self.df = dataframe[dataframe['split'] == split].reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "        self.emotion_to_id = {emotion: idx for idx, emotion in enumerate(config['emotion_labels'].keys())}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def load_and_resize_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Load and resize image (no face detection - saves GPU memory)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img = cv2.imread(str(image_path))\n",
    "            if img is None:\n",
    "                return None\n",
    "            \n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Simple resize - no face detection needed (dataset pre-cropped)\n",
    "            img_resized = cv2.resize(img_rgb, self.config['image_size'])\n",
    "            return img_resized\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Extract emotion from image path (folder name)\n",
    "        # Path structure: .../split/emotion/image.jpg\n",
    "        image_path = row['image_path']\n",
    "        emotion_folder = Path(image_path).parent.name  # Get emotion folder name\n",
    "        \n",
    "        # Load and resize image (no face detection)\n",
    "        image = self.load_and_resize_image(image_path)\n",
    "        if image is None:\n",
    "            image = np.zeros((*self.config['image_size'], 3), dtype=np.uint8)\n",
    "        \n",
    "        # Get emotion label from folder name\n",
    "        emotion_label = self.config['emotion_labels'].get(emotion_folder, 'Unknown')\n",
    "        \n",
    "        # Process with BLIP processor\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=f\"emotion: {emotion_label}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].squeeze(0)\n",
    "        \n",
    "        inputs['emotion_label'] = emotion_label\n",
    "        inputs['emotion_id'] = self.emotion_to_id.get(emotion_folder, 0)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "print(\"âœ“ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075082e",
   "metadata": {},
   "source": [
    "## 5. Load BLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model and processor\n",
    "print(f\"Loading BLIP model: {CONFIG['model_name']}\")\n",
    "processor = AutoProcessor.from_pretrained(CONFIG['model_name'])\n",
    "model = BlipForConditionalGeneration.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"âœ“ Gradient checkpointing enabled\")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(CONFIG['device'])\n",
    "model.to(device)\n",
    "\n",
    "print(f\"âœ“ Model loaded\")\n",
    "print(f\"âœ“ Model device: {device}\")\n",
    "print(f\"âœ“ Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de570875",
   "metadata": {},
   "source": [
    "## 6. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = FERDataset(dataset_df, processor, CONFIG, split='train')\n",
    "val_dataset = FERDataset(dataset_df, processor, CONFIG, split='val')\n",
    "test_dataset = FERDataset(dataset_df, processor, CONFIG, split='test')\n",
    "\n",
    "print(f\"âœ“ Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"âœ“ Val dataset: {len(val_dataset)} images\")\n",
    "print(f\"âœ“ Test dataset: {len(test_dataset)} images\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ DataLoaders created\")\n",
    "print(f\"âœ“ Train batches: {len(train_loader)}\")\n",
    "print(f\"âœ“ Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac52a53",
   "metadata": {},
   "source": [
    "## 7. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d383db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * CONFIG['num_epochs']\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Optimizer: AdamW (lr={CONFIG['learning_rate']})\")\n",
    "print(f\"âœ“ Scheduler: CosineAnnealing (total_steps={total_steps})\")\n",
    "print(f\"âœ“ Training will run for {CONFIG['num_epochs']} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20145bd7",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc802e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, device, epoch, config):\n",
    "    \"\"\"\n",
    "    Train for one epoch with gradient accumulation and mixed precision\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "    batch_times = []\n",
    "    \n",
    "    # Setup automatic mixed precision if enabled\n",
    "    scaler = torch.amp.GradScaler('cuda') if config.get('use_fp16', False) else None\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Move to device\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if config.get('use_fp16', False):\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "                loss = outputs.loss / config['accumulation_steps']\n",
    "        else:\n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            loss = outputs.loss / config['accumulation_steps']\n",
    "        \n",
    "        # Backward pass with gradient accumulation\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # Update weights every accumulation_steps\n",
    "        if (batch_idx + 1) % config['accumulation_steps'] == 0:\n",
    "            if scaler:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Aggressive GPU memory cleanup\n",
    "            if (batch_idx + 1) % (config['accumulation_steps'] * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss += loss.item() * config['accumulation_steps']\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Log every N batches\n",
    "        if (batch_idx + 1) % config['log_interval'] == 0:\n",
    "            avg_batch_time = np.mean(batch_times[-config['log_interval']:])\n",
    "            remaining_batches = len(train_loader) - batch_idx - 1\n",
    "            eta_seconds = avg_batch_time * remaining_batches\n",
    "            eta_minutes = eta_seconds / 60\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{(loss.item() * config[\"accumulation_steps\"]):.4f}',\n",
    "                'batch_time': f'{batch_time:.2f}s',\n",
    "                'eta': f'{eta_minutes:.1f}m'\n",
    "            })\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    return avg_loss, epoch_time\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validate model with timing\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    val_start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "    \n",
    "    val_time = time.time() - val_start_time\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)print(\"âœ“ Training functions defined\")\n",
    "\n",
    "    return avg_loss, val_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a6e40",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d145e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Clear GPU cache before training\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"âœ“ GPU cache cleared\")\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting BLIP Fine-tuning on RAF-DB Dataset\")\n",
    "print(f\"Image Size: {CONFIG['image_size']}\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']} (effective: {CONFIG['batch_size'] * CONFIG['accumulation_steps']})\")\n",
    "print(f\"Gradient Accumulation Steps: {CONFIG['accumulation_steps']}\")\n",
    "print(f\"Mixed Precision (FP16): {CONFIG.get('use_fp16', False)}\")\n",
    "print(f\"Num Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'epoch_time': [],\n",
    "    'val_time': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epoch_logs = []\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    epoch_log = {'epoch': epoch + 1}\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, epoch_time = train_epoch(model, train_loader, optimizer, scheduler, device, epoch, CONFIG)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['epoch_time'].append(epoch_time)\n",
    "    epoch_log['train_loss'] = train_loss\n",
    "    epoch_log['epoch_time'] = epoch_time\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history['learning_rate'].append(current_lr)\n",
    "    epoch_log['lr'] = current_lr\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Time: {epoch_time:.1f}s ({epoch_time/60:.2f}m)\")\n",
    "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Clear GPU cache before validation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_time = validate(model, val_loader, device)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_time'].append(val_time)\n",
    "    epoch_log['val_loss'] = val_loss\n",
    "    epoch_log['val_time'] = val_time\n",
    "    \n",
    "    print(f\"Val Loss: {val_loss:.4f} | Time: {val_time:.1f}s ({val_time/60:.2f}m)\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_path = Path(CONFIG['models_path']) / f\"blip_best_epoch_{epoch+1}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"âœ“ Saved best model: {model_path.name}\")\n",
    "        epoch_log['best_model'] = True\n",
    "    \n",
    "    epoch_logs.append(epoch_log)\n",
    "    \n",
    "    # Calculate ETA\n",
    "    elapsed_time = time.time() - training_start_time\n",
    "    avg_epoch_time = elapsed_time / (epoch + 1)\n",
    "    remaining_epochs = CONFIG['num_epochs'] - (epoch + 1)\n",
    "    eta_seconds = avg_epoch_time * remaining_epochs\n",
    "    eta_minutes = eta_seconds / 60\n",
    "    eta_hours = eta_minutes / 60\n",
    "    \n",
    "    print(f\"Total Elapsed: {elapsed_time/3600:.2f}h | ETA: {eta_hours:.2f}h\")\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"Average Time per Epoch: {total_training_time/CONFIG['num_epochs']:.1f} seconds\")\n",
    "print(f\"Total Training Time: {total_training_time/3600:.2f} hours ({total_training_time/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d9250",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b71f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Evaluating on test set...\")\n",
    "test_loss, test_time = validate(model, test_loader, device)\n",
    "print(f\"\\nâœ“ Test Loss: {test_loss:.4f}\")\n",
    "print(f\"âœ“ Test Evaluation Time: {test_time:.1f}s ({test_time/60:.2f}m)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915bbbd",
   "metadata": {},
   "source": [
    "## 11. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf15e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = Path(CONFIG['logs_path']) / f\"training_history_{CONFIG['timestamp']}.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Training history saved: {history_path}\")\n",
    "\n",
    "# Save detailed epoch logs\n",
    "epoch_logs_path = Path(CONFIG['logs_path']) / f\"epoch_logs_{CONFIG['timestamp']}.json\"\n",
    "with open(epoch_logs_path, 'w') as f:\n",
    "    json.dump(epoch_logs, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Epoch logs saved: {epoch_logs_path}\")\n",
    "\n",
    "# Save training config\n",
    "config_path = Path(CONFIG['logs_path']) / f\"training_config_{CONFIG['timestamp']}.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ“ Training config saved: {config_path}\")\n",
    "\n",
    "# Plot training curves with timing\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('BLIP Training History - Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epoch timing\n",
    "axes[0, 1].bar(range(1, len(history['epoch_time']) + 1), history['epoch_time'])\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Time (seconds)')\n",
    "axes[0, 1].set_title('Epoch Training Time')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[1, 0].plot(history['learning_rate'], marker='o', color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Cumulative time\n",
    "cumulative_time = np.cumsum(history['epoch_time'])\n",
    "axes[1, 1].plot(range(1, len(cumulative_time) + 1), cumulative_time/3600, marker='o', color='red')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Cumulative Time (hours)')\n",
    "axes[1, 1].set_title('Cumulative Training Time')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG['logs_path']) / f\"training_curves_{CONFIG['timestamp']}.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d738152",
   "metadata": {},
   "source": [
    "## 13. Save Trained Model for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model state dict and configuration\n",
    "print(\"=\"*70)\n",
    "print(\"Saving Trained Model\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Save best model state dict\n",
    "best_model_path = Path(CONFIG['models_path']) / f\"blip_fer_best_model.pt\"\n",
    "torch.save(model.state_dict(), best_model_path)\n",
    "print(f\"âœ“ Best model state dict saved: {best_model_path}\")\n",
    "\n",
    "# Save complete model (including architecture)\n",
    "full_model_path = Path(CONFIG['models_path']) / f\"blip_fer_full_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'emotion_labels': CONFIG['emotion_labels'],\n",
    "    'timestamp': CONFIG['timestamp']\n",
    "}, full_model_path)\n",
    "print(f\"âœ“ Full model with config saved: {full_model_path}\")\n",
    "\n",
    "# Save processor for later use\n",
    "processor_path = Path(CONFIG['models_path']) / f\"blip_processor\"\n",
    "processor.save_pretrained(processor_path)\n",
    "print(f\"âœ“ Processor saved: {processor_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': CONFIG['model_name'],\n",
    "    'training_timestamp': CONFIG['timestamp'],\n",
    "    'best_val_loss': float(best_val_loss),\n",
    "    'num_epochs_trained': CONFIG['num_epochs'],\n",
    "    'image_size': CONFIG['image_size'],\n",
    "    'emotion_labels': CONFIG['emotion_labels'],\n",
    "    'total_training_time_hours': total_training_time / 3600,\n",
    "    'training_history': {\n",
    "        'train_loss': history['train_loss'],\n",
    "        'val_loss': history['val_loss'],\n",
    "        'epoch_times': history['epoch_time']\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = Path(CONFIG['models_path']) / f\"model_metadata_{CONFIG['timestamp']}.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"âœ“ Model metadata saved: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"All model files saved to: {CONFIG['models_path']}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# List all saved files\n",
    "print(\"Saved files:\")\n",
    "for file in sorted(Path(CONFIG['models_path']).glob('*')):\n",
    "    if file.is_file():\n",
    "        size_mb = file.stat().st_size / (1024**2)\n",
    "        print(f\"  - {file.name} ({size_mb:.2f} MB)\")\n",
    "    elif file.is_dir():\n",
    "        print(f\"  - {file.name}/ (directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4037b31",
   "metadata": {},
   "source": [
    "## 14. Load Trained Model (for inference in future sessions)\n",
    "\n",
    "To use the trained model in a future session, run the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load trained model for inference\n",
    "print(\"=\"*70)\n",
    "print(\"Loading Trained Model\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Load processor\n",
    "loaded_processor = AutoProcessor.from_pretrained(CONFIG['models_path'] / 'blip_processor')\n",
    "print(f\"âœ“ Processor loaded\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = BlipForConditionalGeneration.from_pretrained(CONFIG['model_name'])\n",
    "loaded_model.gradient_checkpointing_enable()\n",
    "\n",
    "# Load best model state dict\n",
    "model_path = Path(CONFIG['models_path']) / 'blip_fer_best_model.pt'\n",
    "loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "print(f\"âœ“ Model weights loaded from: {model_path}\")\n",
    "\n",
    "# Load metadata\n",
    "metadata_files = sorted(Path(CONFIG['models_path']).glob('model_metadata_*.json'))\n",
    "if metadata_files:\n",
    "    with open(metadata_files[-1], 'r') as f:\n",
    "        loaded_metadata = json.load(f)\n",
    "    print(f\"âœ“ Model metadata loaded\")\n",
    "    print(f\"\\n  Best validation loss: {loaded_metadata['best_val_loss']:.4f}\")\n",
    "    print(f\"  Training time: {loaded_metadata['total_training_time_hours']:.2f} hours\")\n",
    "    print(f\"  Image size: {loaded_metadata['image_size']}\")\n",
    "    print(f\"  Emotions: {', '.join(loaded_metadata['emotion_labels'].values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example with loaded model\n",
    "def predict_emotion_inference(model, processor, image_path, device, emotion_labels):\n",
    "    \"\"\"\n",
    "    Predict emotion from image using loaded model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (64, 64))  # Match training size\n",
    "    \n",
    "    # Process\n",
    "    inputs = processor(images=img_resized, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        caption_ids = model.generate(pixel_values=pixel_values, max_length=20)\n",
    "        caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Test inference on sample\n",
    "if len(test_dataset) > 0:\n",
    "    sample_path = test_dataset.df.iloc[0]['image_path']\n",
    "    prediction = predict_emotion_inference(loaded_model, loaded_processor, sample_path, device, CONFIG['emotion_labels'])\n",
    "    print(f\"\\nSample inference:\")\n",
    "    print(f\"  Image: {Path(sample_path).name}\")\n",
    "    print(f\"  Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174a4e6",
   "metadata": {},
   "source": [
    "## 15. Model Checkpoints Summary\n",
    "\n",
    "**Saved Model Files:**\n",
    "\n",
    "1. **blip_fer_best_model.pt** - Best model weights (state dict only, smaller file)\n",
    "   - Use this for inference by loading weights into a fresh model\n",
    "   - Smallest file size (~900 MB)\n",
    "\n",
    "2. **blip_fer_full_model.pt** - Complete model snapshot\n",
    "   - Includes model weights, config, and metadata\n",
    "   - Easier one-stop loading\n",
    "   - Larger file size (~1.2 GB)\n",
    "\n",
    "3. **blip_processor/** - BLIP image processor\n",
    "   - Required for preprocessing images before inference\n",
    "   - Can be loaded with `AutoProcessor.from_pretrained()`\n",
    "\n",
    "4. **model_metadata_*.json** - Training metadata\n",
    "   - Training loss history\n",
    "   - Best validation loss\n",
    "   - Training duration\n",
    "   - Emotion labels mapping\n",
    "\n",
    "**To Load Model in Future Session:**\n",
    "```python\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained('path_to_processor')\n",
    "\n",
    "# Load model\n",
    "model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "model.load_state_dict(torch.load('path_to_best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Use model for inference\n",
    "```\n",
    "\n",
    "All files are saved to your Google Drive in the `models/` directory and will persist after the session closes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609607b",
   "metadata": {},
   "source": [
    "## 16. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210b285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a sample image\n",
    "def predict_emotion(model, processor, image_path, device):\n",
    "    \"\"\"\n",
    "    Predict emotion from image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (224, 224))\n",
    "    \n",
    "    # Process\n",
    "    inputs = processor(images=img_resized, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        caption_ids = model.generate(pixel_values=pixel_values, max_length=20)\n",
    "        caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Test on sample\n",
    "if len(test_dataset) > 0:\n",
    "    sample_path = test_dataset.df.iloc[0]['image_path']\n",
    "    prediction = predict_emotion(model, processor, sample_path, device)\n",
    "    print(f\"Sample prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a39ccf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ“ BLIP model successfully fine-tuned on RAF-DB dataset  \n",
    "âœ“ Best model saved to `models/` directory  \n",
    "âœ“ Training history and curves saved to `logs/` directory  \n",
    "âœ“ Ready for inference and deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
