{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e01c0e",
   "metadata": {},
   "source": [
    "# FER Data Preparation - Process and Save to Drive\n",
    "This notebook handles all data preprocessing steps and saves the final dataset to Google Drive for use in training.\n",
    "\n",
    "**Output:** Processed images and balanced dataset JSON saved to `/content/drive/MyDrive/processed_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install Mediapipe for high-accuracy facial landmarks\n",
    "!pip install mediapipe\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c8c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install mediapipe\n",
    "\n",
    "# This forced refresh tells the notebook to look at the folders again\n",
    "import site\n",
    "from importlib import reload\n",
    "reload(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1595f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "class SimpleRAFPreprocessor:\n",
    "    def __init__(self, output_size=(336, 336)):\n",
    "        self.output_size = output_size\n",
    "        # Load OpenCV's pre-trained face and eye detectors\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "    def align_and_crop(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return cv2.resize(image, self.output_size)\n",
    "\n",
    "        # Take the largest face found\n",
    "        (x, y, w, h) = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)[0]\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = image[y:y+h, x:x+w]\n",
    "\n",
    "        # Detect eyes within the face ROI\n",
    "        eyes = self.eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(eyes) >= 2:\n",
    "            # Sort eyes by x-coordinate to identify left vs right\n",
    "            eyes = sorted(eyes, key=lambda e: e[0])\n",
    "            left_eye_center = (eyes[0][0] + eyes[0][2]//2, eyes[0][1] + eyes[0][3]//2)\n",
    "            right_eye_center = (eyes[1][0] + eyes[1][2]//2, eyes[1][1] + eyes[1][3]//2)\n",
    "\n",
    "            # Calculate angle\n",
    "            dY = right_eye_center[1] - left_eye_center[1]\n",
    "            dX = right_eye_center[0] - left_eye_center[0]\n",
    "            angle = np.degrees(np.arctan2(dY, dX))\n",
    "\n",
    "            # Rotate\n",
    "            center = (int(w / 2), int(h / 2))\n",
    "            M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            rotated = cv2.warpAffine(roi_color, M, (w, h))\n",
    "            return cv2.resize(rotated, self.output_size)\n",
    "\n",
    "        # Fallback: Just crop the face box if eyes aren't found\n",
    "        face_crop = image[y:y+h, x:x+w]\n",
    "        return cv2.resize(face_crop, self.output_size)\n",
    "\n",
    "# --- Initialize ---\n",
    "preprocessor = SimpleRAFPreprocessor()\n",
    "print(\"‚úÖ Preprocessor initialized using OpenCV (Failsafe Mode).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the base paths for your shortcuts\n",
    "base_paths = {\n",
    "    \"Compound\": \"/content/drive/MyDrive/compound\",\n",
    "    \"RAF-AU\": \"/content/drive/MyDrive/RAF-AU\",\n",
    "    \"RAF-ML\": \"/content/drive/MyDrive/RAF-ML\"\n",
    "}\n",
    "\n",
    "def inspect_data(base_dirs):\n",
    "    print(\"üîç --- RAF Dataset Inspection --- üîç\\n\")\n",
    "    for name, path in base_dirs.items():\n",
    "        print(f\"üìÅ Folder: {name} ({path})\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"   ‚ùå Path does not exist. Check shortcut name/location.\")\n",
    "            continue\n",
    "\n",
    "        # List first 5 items to see directory structure\n",
    "        try:\n",
    "            items = os.listdir(path)\n",
    "            print(f\"   Items found: {len(items)}\")\n",
    "            print(f\"   Sample contents: {items[:5]}\")\n",
    "\n",
    "            # Check for common subfolders like 'Image' or 'Annotation'\n",
    "            for sub in items:\n",
    "                sub_path = os.path.join(path, sub)\n",
    "                if os.path.isdir(sub_path):\n",
    "                    sub_items = os.listdir(sub_path)\n",
    "                    print(f\"     ‚îî‚îÄ üìÇ {sub}/ ({len(sub_items)} items)\")\n",
    "                    if sub_items:\n",
    "                        print(f\"        ‚îî‚îÄ Sample: {sub_items[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error reading folder: {e}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "inspect_data(base_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define where to extract\n",
    "extract_path = '/content/raf_data_unzipped'\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# List of zip files to extract based on your inspection\n",
    "zips_to_extract = [\n",
    "    ('/content/drive/MyDrive/RAF-AU/aligned.zip', 'RAF-AU-aligned'),\n",
    "    ('/content/drive/MyDrive/RAF-ML/Image/aligned.zip', 'RAF-ML-aligned'),\n",
    "    # Add paths for Compound zips if they appeared in your 'Image' folders\n",
    "]\n",
    "\n",
    "for zip_path, folder_name in zips_to_extract:\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"üì¶ Unzipping {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(extract_path, folder_name))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Could not find {zip_path}\")\n",
    "\n",
    "print(\"‚úÖ Unzipping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0670c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Compound/ML Label Preview ---\")\n",
    "!head -n 5 /content/drive/MyDrive/RAF-ML/EmoLabel/partition_label.txt\n",
    "\n",
    "print(\"\\n--- RAF-AU Label Preview ---\")\n",
    "!head -n 5 /content/drive/MyDrive/RAF-AU/RAFAU_label.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f72cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create local directories\n",
    "!mkdir -p /content/raf_images\n",
    "!mkdir -p /content/raf_annotations\n",
    "\n",
    "# Unzip the Action Unit images and the Annotation zips from your Drive shortcuts\n",
    "zips = {\n",
    "    '/content/drive/MyDrive/RAF-AU/aligned.zip': '/content/raf_images/',\n",
    "    '/content/drive/MyDrive/RAF-ML/Annotation/manual.zip': '/content/raf_annotations/'\n",
    "}\n",
    "\n",
    "for zip_path, target in zips.items():\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(target)\n",
    "            print(f\"‚úÖ Extracted {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "EMO_PATH = '/content/drive/MyDrive/RAF-ML/EmoLabel/multilabel.txt'  # Multi-label emotions (6D)\n",
    "AU_PATH = '/content/drive/MyDrive/RAF-AU/RAFAU_label.txt'\n",
    "\n",
    "# Emotion names mapping\n",
    "EMOTION_NAMES = ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger']\n",
    "\n",
    "def create_dataset_json():\n",
    "    # 1. Load AUs into a dictionary\n",
    "    au_map = {}\n",
    "    with open(AU_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()  # Use split() without args to handle multiple spaces\n",
    "            if len(parts) >= 2:\n",
    "                au_map[parts[0]] = parts[1]  # e.g., {'0001.jpg': '1+4+25'}\n",
    "\n",
    "    # 2. Load Multi-label Emotions and Merge\n",
    "    final_data = []\n",
    "    with open(EMO_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()  # Use split() without args to handle multiple spaces\n",
    "            \n",
    "            if len(parts) < 7:  # Need img_id + 6 emotion dimensions\n",
    "                continue\n",
    "            \n",
    "            img_id = parts[0]\n",
    "            \n",
    "            # Parse the 6-dimensional emotion vector (skip empty strings)\n",
    "            try:\n",
    "                emotion_vector = [int(parts[i]) for i in range(1, 7)]\n",
    "            except (ValueError, IndexError):\n",
    "                print(f\"‚ö†Ô∏è Skipping {img_id}: Invalid emotion data - {parts[1:7]}\")\n",
    "                continue\n",
    "            \n",
    "            # Only include if we have AU data\n",
    "            au_val = au_map.get(img_id, \"null\")\n",
    "            if au_val != \"null\":\n",
    "                # Identify which emotions are present\n",
    "                present_emotions = [EMOTION_NAMES[i] for i in range(6) if emotion_vector[i] == 1]\n",
    "                emotion_label = ', '.join(present_emotions) if present_emotions else 'Neutral'\n",
    "                \n",
    "                entry = {\n",
    "                    \"id\": img_id,\n",
    "                    \"image\": f\"aligned_faces/{img_id}\",\n",
    "                    \"emotion_vector\": emotion_vector,  # [Surprise, Fear, Disgust, Happiness, Sadness, Anger]\n",
    "                    \"emotions_present\": present_emotions,\n",
    "                    \"conversations\": [\n",
    "                        {\n",
    "                            \"from\": \"human\",\n",
    "                            \"value\": \"<image>\\nIdentify the emotions present and list the facial Action Units (AUs) involved.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"from\": \"gpt\",\n",
    "                            \"value\": f\"This face exhibits {emotion_label}. The observed facial cues correspond to Action Units: {au_val}.\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                final_data.append(entry)\n",
    "\n",
    "    with open('dataset_vision_llm.json', 'w') as f:\n",
    "        json.dump(final_data, f, indent=2)\n",
    "    print(f\"üéâ Success! Created dataset_vision_llm.json with {len(final_data)} entries.\")\n",
    "    print(f\"Each image now has multi-label emotion identification.\")\n",
    "\n",
    "create_dataset_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset you just created\n",
    "with open('dataset_vision_llm.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total Dataset Entries: {len(data)}\\n\")\n",
    "\n",
    "# Analyze emotion distribution\n",
    "EMOTION_NAMES = ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger']\n",
    "emotion_counts = {emotion: 0 for emotion in EMOTION_NAMES}\n",
    "\n",
    "for entry in data:\n",
    "    for emotion in entry['emotions_present']:\n",
    "        emotion_counts[emotion] += 1\n",
    "\n",
    "# Also count multi-emotion images\n",
    "single_emotion = sum(1 for entry in data if len(entry['emotions_present']) == 1)\n",
    "multi_emotion = sum(1 for entry in data if len(entry['emotions_present']) > 1)\n",
    "neutral = sum(1 for entry in data if len(entry['emotions_present']) == 0)\n",
    "\n",
    "print(\"Emotion Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "for emotion, count in emotion_counts.items():\n",
    "    percentage = (count / len(data)) * 100\n",
    "    print(f\"{emotion:12} : {count:4} occurrences ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"Single Emotion Images  : {single_emotion}\")\n",
    "print(f\"Multi-Emotion Images   : {multi_emotion}\")\n",
    "print(f\"Neutral/None           : {neutral}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of emotion frequencies\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F']\n",
    "ax1.bar(emotion_counts.keys(), emotion_counts.values(), color=colors)\n",
    "ax1.set_title('Emotion Distribution in RAF-ML', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Emotions')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart of emotion complexity\n",
    "complexity = [single_emotion, multi_emotion, neutral]\n",
    "labels = [f'Single\\n({single_emotion})', f'Multi-label\\n({multi_emotion})', f'Neutral\\n({neutral})']\n",
    "colors_pie = ['#3498db', '#e74c3c', '#95a5a6']\n",
    "ax2.pie(complexity, labels=labels, autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "ax2.set_title('Emotion Complexity Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample entries\n",
    "print(\"\\nüì∏ Sample Entries:\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(3, len(data))):\n",
    "    print(f\"\\nImage: {data[i]['id']}\")\n",
    "    print(f\"Emotions: {data[i]['emotions_present']}\")\n",
    "    print(f\"Emotion Vector: {data[i]['emotion_vector']}\")\n",
    "    print(f\"Response: {data[i]['conversations'][1]['value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "# Define the transformations required by your plan\n",
    "transform = A.Compose([\n",
    "    A.Rotate(limit=20, p=0.5),             # Rotations\n",
    "    A.RandomBrightnessContrast(p=0.5),    # Lighting adjustments\n",
    "    A.CoarseDropout(max_holes=1, max_height=40, max_width=40, p=0.3) # Light occlusions\n",
    "])\n",
    "\n",
    "# Example of how to apply to a minority class image\n",
    "def augment_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    augmented = transform(image=image)['image']\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def balance_dataset(json_data):\n",
    "    # Separate the classes\n",
    "    class_0 = [d for d in json_data if d['conversations'][1]['value'].split('label is ')[1].startswith('0')]\n",
    "    class_1 = [d for d in json_data if d['conversations'][1]['value'].split('label is ')[1].startswith('1')]\n",
    "\n",
    "    # Simple Oversampling: Duplicate Class 1 entries to close the gap\n",
    "    # In a real pipeline, these would be the 'augmented' versions\n",
    "    shortfall = len(class_0) - len(class_1)\n",
    "    extra_class_1 = random.choices(class_1, k=shortfall)\n",
    "\n",
    "    balanced_data = class_0 + class_1 + extra_class_1\n",
    "    random.shuffle(balanced_data)\n",
    "\n",
    "    print(f\"New Dataset Size: {len(balanced_data)}\")\n",
    "    print(f\"Class 0: {len(class_0)} | Class 1: {len(class_1) + shortfall}\")\n",
    "    return balanced_data\n",
    "\n",
    "# Use your existing data\n",
    "# balanced_json = balance_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "# Updated to use the correct argument names for current Albumentations versions\n",
    "transform = A.Compose([\n",
    "    A.Rotate(limit=20, p=0.5),             # Rotations (Requirement 4)\n",
    "    A.RandomBrightnessContrast(p=0.5),    # Lighting adjustments (Requirement 4)\n",
    "    A.CoarseDropout(\n",
    "        num_holes_range=(1, 1),\n",
    "        hole_height_range=(20, 40),\n",
    "        hole_width_range=(20, 40),\n",
    "        p=0.3\n",
    "    ) # Light occlusions (Requirement 4)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# 1. Extract images from your Drive shortcut\n",
    "zip_path = '/content/drive/MyDrive/RAF-AU/aligned.zip'\n",
    "extract_to = '/content/temp_raw_images/'\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"‚úÖ Step A: Images extracted from ZIP.\")\n",
    "else:\n",
    "    print(\"‚ùå Error: ZIP file not found. Check your Drive shortcut.\")\n",
    "\n",
    "# 2. Process them into the aligned_faces folder\n",
    "PROCESSED_DIR = '/content/aligned_faces/'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Find the folder inside the zip (it often creates a subfolder)\n",
    "raw_folder = extract_to\n",
    "for root, dirs, files in os.walk(extract_to):\n",
    "    if any(f.endswith('.jpg') for f in files):\n",
    "        raw_folder = root\n",
    "        break\n",
    "\n",
    "all_imgs = [f for f in os.listdir(raw_folder) if f.endswith('.jpg')]\n",
    "print(f\"‚úÖ Step B: Found {len(all_imgs)} images. Starting alignment...\")\n",
    "\n",
    "for img_name in all_imgs[:500]: # Processing first 500 for a quick test\n",
    "    img = cv2.imread(os.path.join(raw_folder, img_name))\n",
    "    if img is not None:\n",
    "        # Using your OpenCV preprocessor\n",
    "        aligned_face = preprocessor.align_and_crop(img)\n",
    "        cv2.imwrite(os.path.join(PROCESSED_DIR, img_name), aligned_face)\n",
    "\n",
    "print(f\"‚úÖ Step C: {len(os.listdir(PROCESSED_DIR))} faces ready in /content/aligned_faces/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "processed_images = [f for f in os.listdir('/content/aligned_faces/') if f.endswith('.jpg')]\n",
    "sample = random.sample(processed_images, 4)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, img_name in enumerate(sample):\n",
    "    img = cv2.imread(f'/content/aligned_faces/{img_name}')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Aligned: {img_name}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee88768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('dataset_vision_llm.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total Linked Entries: {len(data)}\")\n",
    "print(\"Sample Mapping Structure:\")\n",
    "print(json.dumps(data[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63380376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check to process EVERY image in your JSON mapping\n",
    "processed_count = 0\n",
    "for entry in data:\n",
    "    img_id = entry['id']\n",
    "    target_path = os.path.join(PROCESSED_DIR, img_id)\n",
    "\n",
    "    # Only process if the file doesn't already exist in the aligned folder\n",
    "    if not os.path.exists(target_path):\n",
    "        source_path = os.path.join(raw_folder, img_id)\n",
    "        img = cv2.imread(source_path)\n",
    "        if img is not None:\n",
    "            aligned_face = preprocessor.align_and_crop(img)\n",
    "            cv2.imwrite(target_path, aligned_face)\n",
    "            processed_count += 1\n",
    "\n",
    "print(f\"‚úÖ Cleanup complete. Total images in aligned folder: {len(os.listdir(PROCESSED_DIR))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the actual directory containing the .jpg files\n",
    "!find /content/temp_raw_images/ -name \"*.jpg\" | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# 1. Correct Paths\n",
    "SOURCE_FOLDER = '/content/temp_raw_images/aligned/'\n",
    "PROCESSED_DIR = '/content/aligned_faces/'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Process all images in that folder\n",
    "all_files = [f for f in os.listdir(SOURCE_FOLDER) if f.endswith('.jpg')]\n",
    "print(f\"üìÇ Found {len(all_files)} source images. Starting final alignment...\")\n",
    "\n",
    "processed_filenames = set()\n",
    "for img_name in all_files:\n",
    "    img = cv2.imread(os.path.join(SOURCE_FOLDER, img_name))\n",
    "    if img is not None:\n",
    "        # Using your preprocessor with the int-fix\n",
    "        aligned = preprocessor.align_and_crop(img)\n",
    "        cv2.imwrite(os.path.join(PROCESSED_DIR, img_name), aligned)\n",
    "        processed_filenames.add(img_name)\n",
    "\n",
    "print(f\"‚úÖ {len(processed_filenames)} images processed into {PROCESSED_DIR}\")\n",
    "\n",
    "# 3. Update the JSON Mapping to match the new filenames\n",
    "with open('dataset_vision_llm.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "updated_data = []\n",
    "for entry in data:\n",
    "    # Logic: 0001.jpg in JSON becomes 0001_aligned.jpg on disk\n",
    "    original_id = entry['id'].replace('.jpg', '')\n",
    "    new_id = f\"{original_id}_aligned.jpg\"\n",
    "\n",
    "    if new_id in processed_filenames:\n",
    "        entry['id'] = new_id\n",
    "        entry['image'] = f\"aligned_faces/{new_id}\"\n",
    "        updated_data.append(entry)\n",
    "\n",
    "with open('dataset_vision_llm_final.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)\n",
    "\n",
    "print(f\"üéâ FINAL JSON CREATED: 'dataset_vision_llm_final.json' with {len(updated_data)} verified links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41381d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def audit_phase_one(json_path, image_dir):\n",
    "    print(\"üîç --- COUCHE 1: CRITICAL AUDIT START (Multi-Label) --- üîç\\n\")\n",
    "\n",
    "    # 1. JSON & LINKAGE CHECK\n",
    "    if not os.path.exists(json_path):\n",
    "        print(\"‚ùå ERROR: JSON mapping file missing.\")\n",
    "        return\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    total_expected = len(data)\n",
    "\n",
    "    # 2. PHYSICAL FILE CHECK\n",
    "    existing_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "    found_count = len(existing_files)\n",
    "\n",
    "    # 3. DISTRIBUTION CHECK (Requirement: Distribution des classes multi-label)\n",
    "    EMOTION_NAMES = ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger']\n",
    "    emotion_counts = {emotion: 0 for emotion in EMOTION_NAMES}\n",
    "    single_emotion = 0\n",
    "    multi_emotion = 0\n",
    "    \n",
    "    for entry in data:\n",
    "        if len(entry['emotions_present']) == 1:\n",
    "            single_emotion += 1\n",
    "        elif len(entry['emotions_present']) > 1:\n",
    "            multi_emotion += 1\n",
    "        for emotion in entry['emotions_present']:\n",
    "            emotion_counts[emotion] += 1\n",
    "\n",
    "    # 4. NORMALIZATION & CROPPING CHECK (Requirement: D√©tection et recadrage)\n",
    "    sample_img_path = os.path.join(image_dir, existing_files[0])\n",
    "    sample_img = cv2.imread(sample_img_path)\n",
    "    height, width = sample_img.shape[:2]\n",
    "\n",
    "    # 5. DATA AUGMENTATION SIMULATION (Requirement: Eclairage, Rotations, Occlusions)\n",
    "    print(\"üõ†Ô∏è Testing Augmentation Robustness...\")\n",
    "    try:\n",
    "        # Test a rotation and occlusion on a sample to ensure logic is ready\n",
    "        rows, cols = sample_img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((cols/2, rows/2), 15, 1) # 15 degree test\n",
    "        aug_test = cv2.warpAffine(sample_img, M, (cols, rows))\n",
    "        cv2.rectangle(aug_test, (20, 20), (60, 60), (0,0,0), -1) # Occlusion test\n",
    "        aug_success = True\n",
    "    except:\n",
    "        aug_success = False\n",
    "\n",
    "    # --- REPORTING ---\n",
    "    print(f\"{'Requirement':<30} | {'Status':<15} | {'Details'}\")\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Mapping Integrity':<30} | {'‚úÖ PASS' if total_expected == found_count else '‚ùå FAIL':<15} | {found_count}/{total_expected} linked\")\n",
    "    print(f\"{'Face Normalization':<30} | {'‚úÖ PASS' if height == width else '‚ö†Ô∏è WARN':<15} | Size: {width}x{height}\")\n",
    "    print(f\"{'Emotion Complexity':<30} | {'‚úÖ DONE':<15} | S:{single_emotion} M:{multi_emotion}\")\n",
    "    print(f\"{'Emotion Distribution':<30} | {'‚úÖ DONE':<15} | {dict(emotion_counts)}\")\n",
    "    print(f\"{'Augmentation Logic':<30} | {'‚úÖ READY' if aug_success else '‚ùå FAIL':<15} | Rot/Occ test passed\")\n",
    "\n",
    "    # Visual Confirmation\n",
    "    print(\"\\nüì∏ Displaying 3 random aligned samples for visual inspection...\")\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(min(3, len(existing_files))):\n",
    "        idx = i if i < len(data) else 0\n",
    "        img = cv2.imread(os.path.join(image_dir, existing_files[i]))\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        emotions_str = ', '.join(data[idx]['emotions_present']) if data[idx]['emotions_present'] else 'Neutral'\n",
    "        plt.title(f\"Emotions: {emotions_str}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Run the audit\n",
    "audit_phase_one('dataset_vision_llm_final.json', '/content/aligned_faces/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import sys\n",
    "!{sys.executable} -m pip install -U albumentations -q\n",
    "\n",
    "# Clean up sys.modules to force fresh import\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('torch')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Reinstall packages to ensure clean state\n",
    "print(\"üîÑ Reinstalling torch and dependencies...\")\n",
    "!pip uninstall -y torch torchvision torchaudio -q\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "!pip install -U albumentations -q\n",
    "\n",
    "print(\"\\n‚úÖ Packages reinstalled successfully\")\n",
    "\n",
    "# Import torch first to establish proper module hierarchy\n",
    "import torch\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Now import albumentations\n",
    "import albumentations as A\n",
    "print(f\"‚úÖ Albumentations version: {A.__version__}\")\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n‚úÖ All imports successful - ready to proceed with augmentation\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "EMOTION_NAMES = ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger']\n",
    "\n",
    "print(\"üîÑ Preparing final balanced dataset and saving to Drive...\\n\")\n",
    "\n",
    "# Define Drive paths\n",
    "DATA_DIR = '/content/drive/MyDrive/processed_data'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'aligned_faces')\n",
    "DATASET_JSON = os.path.join(DATA_DIR, 'dataset_vision_llm_balanced.json')\n",
    "\n",
    "# Clean ALL existing data (Drive directory + local JSON)\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"üóëÔ∏è  Removing existing Drive directory (includes old JSON + images)...\")\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "    print(f\"‚úÖ Old Drive data cleaned\")\n",
    "\n",
    "# Also clean old local balanced JSON if exists\n",
    "if os.path.exists(DATASET_JSON):\n",
    "    print(f\"üóëÔ∏è  Removing old local balanced JSON file...\")\n",
    "    os.remove(DATASET_JSON)\n",
    "    print(f\"‚úÖ Old local JSON cleaned\")\n",
    "\n",
    "# Create fresh directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Created fresh Drive directories:\")\n",
    "print(f\"   - {DATA_DIR}\")\n",
    "print(f\"   - {IMAGES_DIR}\")\n",
    "\n",
    "# Load the preprocessed data\n",
    "with open('dataset_vision_llm_final.json', 'r') as f:\n",
    "    final_data = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"   - Total entries: {len(final_data)}\")\n",
    "\n",
    "# Convert to simplified Q&A format for final balanced dataset\n",
    "def convert_to_qa_format(entry):\n",
    "    \"\"\"Convert original format to question-answer format\"\"\"\n",
    "    # Extract emotion label from conversations\n",
    "    gpt_response = entry['conversations'][1]['value']\n",
    "    \n",
    "    # Extract emotion and AU info\n",
    "    emotion_label = ', '.join(entry['emotions_present']) if entry['emotions_present'] else 'Neutral'\n",
    "    \n",
    "    # Get AU value from the gpt response\n",
    "    au_val = 'null'\n",
    "    if 'Action Units:' in gpt_response:\n",
    "        au_val = gpt_response.split('Action Units: ')[-1].rstrip('.')\n",
    "    \n",
    "    return {\n",
    "        \"id\": entry['id'],\n",
    "        \"image\": f\"aligned_faces/{entry['id']}\",\n",
    "        \"question\": \"Identify the emotions present and list the facial Action Units (AUs) involved.\",\n",
    "        \"answer\": f\"Emotion: {emotion_label}. Action Units: {au_val}\"\n",
    "    }\n",
    "\n",
    "# Convert all entries\n",
    "converted_data = [convert_to_qa_format(entry) for entry in final_data]\n",
    "\n",
    "# Copy aligned images to Drive\n",
    "print(f\"\\nüìÇ Copying aligned images to Drive...\")\n",
    "copied_count = 0\n",
    "for entry in final_data:\n",
    "    img_id = entry['id']\n",
    "    src_path = os.path.join(PROCESSED_DIR, img_id)\n",
    "    dst_path = os.path.join(IMAGES_DIR, img_id)\n",
    "    \n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        copied_count += 1\n",
    "        if copied_count % 100 == 0:\n",
    "            print(f\"   Copied {copied_count} images...\")\n",
    "\n",
    "print(f\"‚úÖ Copied {copied_count} images to {IMAGES_DIR}\")\n",
    "\n",
    "# Save final JSON to Drive\n",
    "with open(DATASET_JSON, 'w') as f:\n",
    "    json.dump(converted_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved dataset JSON to {DATASET_JSON}\")\n",
    "print(f\"   - Converted entries: {len(converted_data)}\")\n",
    "\n",
    "# Display sample entries\n",
    "print(\"\\nüì∏ Sample Entries from Balanced Dataset (Q&A Format):\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(3, len(converted_data))):\n",
    "    print(f\"\\nEntry {i+1}:\")\n",
    "    print(f\"  ID: {converted_data[i]['id']}\")\n",
    "    print(f\"  Image: {converted_data[i]['image']}\")\n",
    "    print(f\"  Question: {converted_data[i]['question']}\")\n",
    "    print(f\"  Answer: {converted_data[i]['answer']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéâ ALL DATA SAVED TO GOOGLE DRIVE!\")\n",
    "print(f\"üìÅ Data Location: {DATA_DIR}\")\n",
    "print(f\"   - Images: {IMAGES_DIR}/ ({copied_count} files)\")\n",
    "print(f\"   - Dataset: dataset_vision_llm_balanced.json ({len(converted_data)} entries)\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: Old Drive directory (with JSON + images) was completely removed and replaced\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
