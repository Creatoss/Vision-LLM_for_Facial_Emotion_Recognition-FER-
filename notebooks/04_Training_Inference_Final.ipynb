{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abab6e97",
      "metadata": {
        "id": "abab6e97"
      },
      "source": [
        "# FER Training & Inference - Load from Drive and Train BLIP-2\n",
        "This notebook loads the preprocessed dataset from Google Drive and trains BLIP-2 model with LoRA adapters.\n",
        "\n",
        "**Input:** Processed images and balanced dataset from `/content/drive/MyDrive/processed_data/`\n",
        "\n",
        "**Output:** Trained model saved to `/content/drive/MyDrive/blip2-emotion-rafce-final/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034a3b73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "034a3b73",
        "outputId": "5749a119-ce90-4cc9-e0f3-963ddd8a1997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted\n",
            "üìÅ Data directory: /content/drive/MyDrive/processed_data\n",
            "üìÅ Images directory: /content/drive/MyDrive/processed_data/aligned_faces\n",
            "üìÅ Output directory: /content/drive/MyDrive/blip2-emotion-rafce-final\n",
            "\n",
            "‚ùå Processed data NOT found!\n",
            "   Please run Notebook 1 (Data Preparation) first.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "DATA_DIR = '/content/drive/MyDrive/processed_data'\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'aligned_faces')\n",
        "DATASET_JSON = os.path.join(DATA_DIR, 'dataset_vision_llm_balanced.json')\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/blip2-emotion-rafce-final'\n",
        "\n",
        "print(f\"‚úÖ Google Drive mounted\")\n",
        "print(f\"üìÅ Data directory: {DATA_DIR}\")\n",
        "print(f\"üìÅ Images directory: {IMAGES_DIR}\")\n",
        "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Verify data exists\n",
        "if os.path.isdir(IMAGES_DIR) and os.path.isfile(DATASET_JSON):\n",
        "    print(f\"\\n‚úÖ Processed data found!\")\n",
        "    print(f\"   - Images: {len(os.listdir(IMAGES_DIR))} files\")\n",
        "    with open(DATASET_JSON, 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "    print(f\"   - Dataset: {len(dataset)} entries\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Processed data NOT found!\")\n",
        "    print(f\"   Please run Notebook 1 (Data Preparation) first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4ff424",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2b4ff424",
        "outputId": "41e6b2f2-04cb-4cc4-b720-cb12a2ab8e07"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import sys\\n\\nprint(\"üì¶ Installing training dependencies...\\n\")\\n!{sys.executable} -m pip install -U bitsandbytes transformers accelerate peft datasets scipy sentencepiece protobuf -q\\n\\nprint(\"\\n‚úÖ Installation complete!\")'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''import sys\n",
        "\n",
        "print(\"üì¶ Installing training dependencies...\\n\")\n",
        "!{sys.executable} -m pip install -U bitsandbytes transformers accelerate peft datasets scipy sentencepiece protobuf -q\n",
        "\n",
        "print(\"\\n‚úÖ Installation complete!\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4dbc23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544,
          "referenced_widgets": [
            "9c8bf485e1e343cf96df7b91f1ccba86",
            "911ba446be324a53931e290dc8fba533",
            "f737ad0454384d57854970feace5ed04",
            "33acedee421944e89ea53a4261c6bb40",
            "95c8c923da7e4435b07de83d832a0d76",
            "58a5287156de4019809649617145696b",
            "3d23fcdb394c4a85a2d51560d340bb64",
            "3d6cf898b25f42a7bc1c3dbac90ac7c3",
            "6265f06b9c80463aacae0e2932a145ff",
            "ab0a0f18b12148a782494af2a88efe07",
            "18e24c494b864cdda35309f08a34db22"
          ]
        },
        "id": "8d4dbc23",
        "outputId": "601d8eee-22e9-44fc-cb2d-89dfbaba6b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üîß SETTING UP BLIP-2 MODEL WITH LORA\n",
            "======================================================================\n",
            "\n",
            "üì¶ Loading BLIP-2 model: Salesforce/blip2-opt-2.7b\n",
            "üì± Device: cpu\n",
            "‚úÖ Processor loaded\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c8bf485e1e343cf96df7b91f1ccba86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-151178639.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load model with FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m model = Blip2ForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5046\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5047\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5048\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5049\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5050\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5468\u001b[0;31m                 \u001b[0m_error_msgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shard_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5469\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error_msgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         disk_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mto_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîß SETTING UP BLIP-2 MODEL WITH LORA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model_id = \"Salesforce/blip2-opt-2.7b\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"\\nüì¶ Loading BLIP-2 model: {model_id}\")\n",
        "print(f\"üì± Device: {device}\")\n",
        "\n",
        "# Load processor\n",
        "processor = Blip2Processor.from_pretrained(model_id)\n",
        "print(f\"‚úÖ Processor loaded\")\n",
        "\n",
        "# Load model with FP16\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Base model loaded\")\n",
        "print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n",
        "\n",
        "# Apply LoRA\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"\\n‚úÖ LoRA configuration applied!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439bbead",
      "metadata": {
        "id": "439bbead"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "\n",
        "class BLIP2EmotionDataset(Dataset):\n",
        "    \"\"\"Dataset for BLIP-2 emotion recognition with Action Units\"\"\"\n",
        "\n",
        "    def __init__(self, json_file, img_dir, processor, max_length=256):\n",
        "        with open(json_file, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.EMOTION_NAMES = ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Load image from Drive\n",
        "        image_path = os.path.join(self.img_dir, item['id'])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Training prompt\n",
        "        prompt = (\n",
        "            \"Analyze this facial image and identify:\\n\"\n",
        "            \"1. Which emotions are present (Surprise, Fear, Disgust, Happiness, Sadness, Anger)\\n\"\n",
        "            \"2. The facial Action Units (AUs) involved\\n\"\n",
        "            \"Please explain the connection between the AUs and the emotions.\"\n",
        "        )\n",
        "\n",
        "        # Target text\n",
        "        emotions_str = ', '.join(item['emotions_present']) if item['emotions_present'] else 'Neutral'\n",
        "        au_info = item['conversations'][1]['value'].split('Action Units: ')[-1] if 'Action Units:' in item['conversations'][1]['value'] else 'N/A'\n",
        "\n",
        "        target_text = (\n",
        "            f\"This face exhibits: {emotions_str}. \"\n",
        "            f\"Emotion vector: {item['emotion_vector']}. \"\n",
        "            f\"Observed Action Units: {au_info}\"\n",
        "        )\n",
        "\n",
        "        # Process inputs\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        targets = self.processor(\n",
        "            text=target_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Remove batch dimension\n",
        "        for key in inputs:\n",
        "            if inputs[key] is not None:\n",
        "                inputs[key] = inputs[key].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": inputs.pixel_values,\n",
        "            \"input_ids\": inputs.input_ids,\n",
        "            \"attention_mask\": inputs.attention_mask,\n",
        "            \"labels\": targets.input_ids[0],\n",
        "        }\n",
        "\n",
        "# Create dataset\n",
        "print(\"=\"*70)\n",
        "print(\"üìä CREATING PYTORCH DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "dataset = BLIP2EmotionDataset(\n",
        "    json_file=DATASET_JSON,\n",
        "    img_dir=IMAGES_DIR,\n",
        "    processor=processor,\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset created with {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972bff81",
      "metadata": {
        "id": "972bff81"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def blip2_collate_fn(batch):\n",
        "    \"\"\"Custom collator for BLIP-2\"\"\"\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=blip2_collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(\"‚úÖ DataLoader created\")\n",
        "print(f\"   Batch size: 4\")\n",
        "print(f\"   Total batches: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c774c8bc",
      "metadata": {
        "id": "c774c8bc"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚öôÔ∏è CONFIGURING TRAINING PARAMETERS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_config = {\n",
        "    \"batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 2,\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"warmup_steps\": 100,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"fp16\": True,\n",
        "}\n",
        "\n",
        "print(f\"\\nüìã Training Configuration:\")\n",
        "for key, value in training_config.items():\n",
        "    print(f\"   {key:<30}: {value}\")\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=training_config['learning_rate'],\n",
        "    weight_decay=training_config['weight_decay']\n",
        ")\n",
        "\n",
        "# Setup scheduler\n",
        "num_training_steps = len(train_loader) * training_config['num_train_epochs']\n",
        "scheduler = LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=1.0,\n",
        "    end_factor=0.0,\n",
        "    total_iters=num_training_steps\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Optimizer configured\")\n",
        "print(f\"   Total training steps: {num_training_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50e1e2b",
      "metadata": {
        "id": "e50e1e2b"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, epoch, device=\"cuda\"):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            max_norm=1.0\n",
        "        )\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "history = []\n",
        "for epoch in range(training_config['num_train_epochs']):\n",
        "    avg_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        epoch,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    history.append(avg_loss)\n",
        "    print(f\"\\nüìä Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Final training loss: {history[-1]:.4f}\")\n",
        "print(f\"Best loss: {min(history):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524da3b0",
      "metadata": {
        "id": "524da3b0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üíæ SAVING MODEL TO GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÅ Save directory: {OUTPUT_DIR}\")\n",
        "print(f\"üìä Training summary:\")\n",
        "print(f\"   - Epochs: {training_config['num_train_epochs']}\")\n",
        "print(f\"   - Final loss: {history[-1]:.4f}\")\n",
        "print(f\"   - Best loss: {min(history):.4f}\")\n",
        "print(f\"   - Dataset size: {len(dataset)} samples\\n\")\n",
        "\n",
        "# Save LoRA adapter weights\n",
        "print(\"‚û°Ô∏è Saving LoRA adapter weights...\")\n",
        "try:\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    print(\"   ‚úÖ Adapter weights saved\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Failed: {e}\")\n",
        "\n",
        "# Save processor\n",
        "print(\"‚û°Ô∏è Saving processor and tokenizer...\")\n",
        "try:\n",
        "    processor.save_pretrained(OUTPUT_DIR)\n",
        "    print(\"   ‚úÖ Processor saved\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Failed: {e}\")\n",
        "\n",
        "# Save metadata\n",
        "print(\"‚û°Ô∏è Saving training metadata...\")\n",
        "metadata = {\n",
        "    \"model_id\": \"Salesforce/blip2-opt-2.7b\",\n",
        "    \"training_config\": training_config,\n",
        "    \"training_history\": {\n",
        "        \"losses\": history,\n",
        "        \"final_loss\": float(history[-1]),\n",
        "        \"best_loss\": float(min(history)),\n",
        "        \"epochs_completed\": len(history)\n",
        "    },\n",
        "    \"dataset_info\": {\n",
        "        \"total_samples\": len(dataset),\n",
        "        \"json_file\": \"dataset_vision_llm_balanced.json\",\n",
        "        \"image_dir\": IMAGES_DIR\n",
        "    },\n",
        "    \"lora_config\": {\n",
        "        \"r\": 16,\n",
        "        \"lora_alpha\": 32,\n",
        "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
        "        \"lora_dropout\": 0.05,\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    with open(os.path.join(OUTPUT_DIR, \"training_metadata.json\"), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(\"   ‚úÖ Training metadata saved\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Failed: {e}\")\n",
        "\n",
        "# Verify files\n",
        "print(\"\\n‚û°Ô∏è Verifying saved files...\")\n",
        "required_files = [\n",
        "    \"adapter_model.safetensors\",\n",
        "    \"adapter_config.json\",\n",
        "    \"tokenizer.json\",\n",
        "    \"processor_config.json\"\n",
        "]\n",
        "\n",
        "for fname in required_files:\n",
        "    fpath = os.path.join(OUTPUT_DIR, fname)\n",
        "    if os.path.exists(fpath):\n",
        "        size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
        "        print(f\"   ‚úÖ {fname:<40} {size_mb:>8.2f} MB\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {fname:<40} MISSING\")\n",
        "\n",
        "print(f\"\\n‚úÖ Model saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c061b8a6",
      "metadata": {
        "id": "c061b8a6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üß™ RUNNING INFERENCE TESTS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Get random test images\n",
        "all_images = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.jpg')]\n",
        "test_samples = random.sample(all_images, min(3, len(all_images)))\n",
        "\n",
        "print(f\"üì∏ Testing on {len(test_samples)} random images\\n\")\n",
        "\n",
        "def generate_prediction(model, processor, image_path, device=\"cuda\"):\n",
        "    \"\"\"Generate emotion prediction\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    prompt = (\n",
        "        \"Analyze this facial image and identify:\\n\"\n",
        "        \"1. Which emotions are present (Surprise, Fear, Disgust, Happiness, Sadness, Anger)\\n\"\n",
        "        \"2. The facial Action Units (AUs) involved\\n\"\n",
        "        \"Please explain the connection between the AUs and the emotions.\"\n",
        "    )\n",
        "\n",
        "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\", truncation=True)\n",
        "    if device == \"cuda\" and torch.cuda.is_available():\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=96,\n",
        "            min_new_tokens=16,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "\n",
        "    prediction = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "    return prediction\n",
        "\n",
        "# Test and visualize\n",
        "fig, axes = plt.subplots(1, len(test_samples), figsize=(15, 5))\n",
        "if len(test_samples) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "predictions = []\n",
        "\n",
        "print(\"=\"*70)\n",
        "for idx, img_name in enumerate(test_samples):\n",
        "    img_path = os.path.join(IMAGES_DIR, img_name)\n",
        "\n",
        "    print(f\"\\nüñºÔ∏è  Image {idx+1}: {img_name}\")\n",
        "\n",
        "    try:\n",
        "        pred = generate_prediction(model, processor, img_path, device=device)\n",
        "        predictions.append(pred)\n",
        "\n",
        "        print(f\"üìù Prediction: {pred[:150]}...\")\n",
        "\n",
        "        # Display image\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(f\"{img_name}\\n{pred[:30]}...\", fontsize=8)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "        predictions.append(f\"[ERROR: {e}]\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"\\n‚úÖ Inference test complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcab094c",
      "metadata": {
        "id": "fcab094c"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üìä EVALUATION METRICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüìà Training Results:\")\n",
        "print(f\"   Final loss: {history[-1]:.4f}\")\n",
        "print(f\"   Best loss: {min(history):.4f}\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n",
        "\n",
        "print(f\"\\n‚úÖ Inference Quality:\")\n",
        "valid_predictions = [p for p in predictions if not p.startswith('[ERROR')]\n",
        "if valid_predictions:\n",
        "    has_keywords = sum(1 for p in valid_predictions if any(word in p.lower() for word in [\"emotion\", \"face\", \"au\", \"action\"]))\n",
        "    print(f\"   Valid predictions: {len(valid_predictions)}/{len(test_samples)}\")\n",
        "    print(f\"   Keyword presence: {has_keywords}/{len(valid_predictions)}\")\n",
        "    print(f\"   Avg output length: {sum(len(p) for p in valid_predictions) / len(valid_predictions):.0f} chars\")\n",
        "else:\n",
        "    print(f\"   No valid predictions\")\n",
        "\n",
        "print(f\"\\nüéâ Training and evaluation complete!\")\n",
        "print(f\"\\nüìÅ Model saved at: {OUTPUT_DIR}\")\n",
        "print(f\"üìÅ Processed data at: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fuIMpG1Z3sP1",
      "metadata": {
        "id": "fuIMpG1Z3sP1"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PART 11: Reload Trained Model from Google Drive\n",
        "# ============================================================\n",
        "import os, gc, torch\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from peft import PeftModel\n",
        "\n",
        "BASE_ID = \"Salesforce/blip2-opt-2.7b\"\n",
        "\n",
        "# Prefer explicit override if provided; otherwise use the training OUTPUT_DIR\n",
        "ADAPTER_DIR = os.environ.get(\"ADAPTER_DIR_OVERRIDE\", OUTPUT_DIR)\n",
        "ADAPTER_DIR = os.path.abspath(ADAPTER_DIR)\n",
        "\n",
        "# Force offline/local loading to avoid HF repo id validation\n",
        "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üì• RELOADING TRAINED MODEL FROM DRIVE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Adapter dir: {ADAPTER_DIR}\")\n",
        "\n",
        "if not os.path.isdir(ADAPTER_DIR):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Adapter directory not found: {ADAPTER_DIR}\\n\"\n",
        "        \"Mount Google Drive and ensure the training save cell has run.\"\n",
        "    )\n",
        "\n",
        "required_adapter_files = [\n",
        "    \"adapter_model.safetensors\",\n",
        "    \"adapter_config.json\",\n",
        "]\n",
        "missing = [f for f in required_adapter_files if not os.path.exists(os.path.join(ADAPTER_DIR, f))]\n",
        "if missing:\n",
        "    raise FileNotFoundError(\n",
        "        \"Missing adapter files in the adapter directory: \" + \", \".join(missing)\n",
        "    )\n",
        "\n",
        "\n",
        "def load_blip2_with_lora(adapter_dir=ADAPTER_DIR, base_id=BASE_ID, device=None, merge_adapters=True):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\n‚û°Ô∏è Loading base model: {base_id}\")\n",
        "    base_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "        base_id,\n",
        "        dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    )\n",
        "\n",
        "    print(f\"‚û°Ô∏è Attaching LoRA adapters from: {adapter_dir}\")\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        adapter_dir,\n",
        "        is_trainable=False,\n",
        "        local_files_only=True,\n",
        "        token=None,\n",
        "        trust_remote_code=False,\n",
        "    )\n",
        "\n",
        "    if merge_adapters:\n",
        "        print(\"‚û°Ô∏è Merging adapters into base model for inference...\")\n",
        "        try:\n",
        "            model = model.merge_and_unload()\n",
        "            print(\"   ‚úÖ Adapters merged\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Merge failed: {e}. Continuing without merge.\")\n",
        "\n",
        "    model.eval()\n",
        "    if device == \"cuda\":\n",
        "        try:\n",
        "            model.to(\"cuda\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"‚û°Ô∏è Loading processor/tokenizer...\")\n",
        "    try:\n",
        "        processor = AutoProcessor.from_pretrained(adapter_dir, local_files_only=True, token=None)\n",
        "        print(\"   ‚úÖ Processor loaded from adapter directory\")\n",
        "    except Exception:\n",
        "        processor = AutoProcessor.from_pretrained(base_id)\n",
        "        print(\"   ‚ö†Ô∏è Fallback: Processor loaded from base model\")\n",
        "\n",
        "    return model, processor, device\n",
        "\n",
        "reloaded_model, reloaded_processor, reload_device = load_blip2_with_lora()\n",
        "print(\"\\n‚úÖ Reload complete. Model ready for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MAgjScJo3t4N",
      "metadata": {
        "id": "MAgjScJo3t4N"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PART 12: Evaluation ‚Äî Exact-Match Accuracy and F1 Scores\n",
        "# ============================================================\n",
        "import os, re, time, json, random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure sklearn is available for metrics\n",
        "try:\n",
        "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "except Exception:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install scikit-learn -q\n",
        "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "EMOTION_CANONICAL = [\"Surprise\", \"Fear\", \"Disgust\", \"Happiness\", \"Sadness\", \"Anger\"]\n",
        "VARIANTS = {\n",
        "    \"Happiness\": [\"happiness\", \"happy\"],\n",
        "    \"Sadness\": [\"sadness\", \"sad\"],\n",
        "    \"Anger\": [\"anger\", \"angry\"],\n",
        "    \"Fear\": [\"fear\", \"afraid\", \"fearful\"],\n",
        "    \"Disgust\": [\"disgust\", \"disgusted\"],\n",
        "    \"Surprise\": [\"surprise\", \"surprised\"],\n",
        "}\n",
        "\n",
        "\n",
        "def extract_predicted_emotions(text: str):\n",
        "    text_l = text.lower()\n",
        "    preds = set()\n",
        "    for canon, variations in VARIANTS.items():\n",
        "        for v in variations:\n",
        "            if re.search(rf\"\\b{re.escape(v)}\\b\", text_l):\n",
        "                preds.add(canon)\n",
        "                break\n",
        "    return preds\n",
        "\n",
        "\n",
        "def evaluate_accuracy(dataset_json=DATASET_JSON, img_dir=IMAGES_DIR, num_samples=100, device=reload_device):\n",
        "    with open(dataset_json, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    subset = random.sample(data, min(num_samples, len(data)))\n",
        "    y_true, y_pred = [], []\n",
        "    exact_match = 0\n",
        "    latencies = []\n",
        "\n",
        "    prompt = (\n",
        "        \"Analyze this facial image and identify:\\n\"\n",
        "        \"1. Which emotions are present (Surprise, Fear, Disgust, Happiness, Sadness, Anger)\\n\"\n",
        "        \"2. The facial Action Units (AUs) involved\\n\"\n",
        "        \"Please explain the connection between the AUs and the emotions.\"\n",
        "    )\n",
        "\n",
        "    for i, item in enumerate(subset):\n",
        "        image_path = os.path.join(img_dir, item[\"id\"])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        inputs = reloaded_processor(images=image, text=prompt, return_tensors=\"pt\", truncation=True)\n",
        "        if device == \"cuda\" and torch.cuda.is_available():\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            gen_ids = reloaded_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=96,\n",
        "                min_new_tokens=16,\n",
        "                do_sample=False,\n",
        "                num_beams=3,\n",
        "                repetition_penalty=1.1,\n",
        "            )\n",
        "        latencies.append(time.time() - start)\n",
        "\n",
        "        text = reloaded_processor.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "        preds = extract_predicted_emotions(text)\n",
        "        trues = set(item[\"emotions_present\"]) if item.get(\"emotions_present\") else set()\n",
        "\n",
        "        y_true.append([1 if e in trues else 0 for e in EMOTION_CANONICAL])\n",
        "        y_pred.append([1 if e in preds else 0 for e in EMOTION_CANONICAL])\n",
        "\n",
        "        if preds == trues:\n",
        "            exact_match += 1\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"Processed {i+1}/{len(subset)} samples...\")\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    micro_f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    micro_prec = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    micro_rec = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    exact_acc = exact_match / len(subset) if subset else 0.0\n",
        "    avg_latency = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "\n",
        "    metrics = {\n",
        "        \"samples\": len(subset),\n",
        "        \"exact_match_accuracy\": float(exact_acc),\n",
        "        \"micro_f1\": float(micro_f1),\n",
        "        \"macro_f1\": float(macro_f1),\n",
        "        \"micro_precision\": float(micro_prec),\n",
        "        \"micro_recall\": float(micro_rec),\n",
        "        \"avg_latency_sec\": avg_latency,\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä RUNNING ACCURACY EVALUATION (subset)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "metrics = evaluate_accuracy(num_samples=100)\n",
        "print(\"Evaluation metrics:\")\n",
        "for k, v in metrics.items():\n",
        "    if isinstance(v, float):\n",
        "        print(f\"- {k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"- {k}: {v}\")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18e24c494b864cdda35309f08a34db22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33acedee421944e89ea53a4261c6bb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab0a0f18b12148a782494af2a88efe07",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_18e24c494b864cdda35309f08a34db22",
            "value": "‚Äá0/2‚Äá[00:00&lt;?,‚Äá?it/s]"
          }
        },
        "3d23fcdb394c4a85a2d51560d340bb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d6cf898b25f42a7bc1c3dbac90ac7c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a5287156de4019809649617145696b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6265f06b9c80463aacae0e2932a145ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "911ba446be324a53931e290dc8fba533": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58a5287156de4019809649617145696b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3d23fcdb394c4a85a2d51560d340bb64",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá‚Äá‚Äá0%"
          }
        },
        "95c8c923da7e4435b07de83d832a0d76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c8bf485e1e343cf96df7b91f1ccba86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_911ba446be324a53931e290dc8fba533",
              "IPY_MODEL_f737ad0454384d57854970feace5ed04",
              "IPY_MODEL_33acedee421944e89ea53a4261c6bb40"
            ],
            "layout": "IPY_MODEL_95c8c923da7e4435b07de83d832a0d76"
          }
        },
        "ab0a0f18b12148a782494af2a88efe07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f737ad0454384d57854970feace5ed04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d6cf898b25f42a7bc1c3dbac90ac7c3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6265f06b9c80463aacae0e2932a145ff",
            "value": 0
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
